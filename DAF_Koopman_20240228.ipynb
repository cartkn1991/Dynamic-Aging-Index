{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc25ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda, BatchNormalization, LeakyReLU, Add, Dropout\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow import keras\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a74523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_title(title):\n",
    "    print(f'{50 * \"=\"}')\n",
    "    print(title)\n",
    "    print(f'{50 * \"=\"}')\n",
    "\n",
    "\n",
    "print_title('Loading Data')\n",
    "df = pq.read_table(\"C:\\\\Users\\\\G20187729\\\\Desktop\\\\Python_codes_3\\\\Data\\\\20240710_crosssectionaldata\\\\X.parquet\", use_threads=True).to_pandas()\n",
    "X0_train = pq.read_table(\"C:\\\\Users\\\\G20187729\\\\Desktop\\\\Python_codes_3\\\\Data\\\\X0_train.parquet\", use_threads=True).to_pandas()\n",
    "X0_test = pq.read_table(\"C:\\\\Users\\\\G20187729\\\\Desktop\\\\Python_codes_3\\\\Data\\\\X0_test.parquet\", use_threads=True).to_pandas()\n",
    "X1_train = pq.read_table(\"C:\\\\Users\\\\G20187729\\\\Desktop\\\\Python_codes_3\\\\Data\\\\X1_train.parquet\", use_threads=True).to_pandas()\n",
    "X1_test = pq.read_table(\"C:\\\\Users\\\\G20187729\\\\Desktop\\\\Python_codes_3\\\\Data\\\\X1_test.parquet\", use_threads=True).to_pandas()\n",
    "print_title('Data loading complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2986693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:, ]\n",
    "y1 = df.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb35736-73ec-41e5-ac11-c196adc92b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X.iloc[:, 0]\n",
    "X = X.iloc[:, 1:, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0c86e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.values.astype(np.float32)\n",
    "X0_train = X0_train.values.astype(np.float32)\n",
    "X1_train = X1_train.values.astype(np.float32)\n",
    "X0_test = X0_test.values.astype(np.float32)\n",
    "X1_test = X1_test.values.astype(np.float32)\n",
    "\n",
    "X_train, X_test, y_train, y_test, y1_train, y1_test = train_test_split(X, y, y1, test_size=0.2, random_state=0)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "X0_train, X0_val = train_test_split(X0_train, test_size=0.2, random_state=0, shuffle=False)\n",
    "X1_train, X1_val = train_test_split(X1_train, test_size=0.2, random_state=0, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3223a3-eae5-440e-9901-de7b3283a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X0_val.shape)\n",
    "print(X1_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e485ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform the training set\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the validation and test sets using the same scaler\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Similarly, fit and transform X0_train, X0_val, X0_test for the second dataset\n",
    "X0_train_scaled = scaler.transform(X0_train)\n",
    "X0_val_scaled = scaler.transform(X0_val)\n",
    "X0_test_scaled = scaler.transform(X0_test)\n",
    "\n",
    "# And fit and transform X1_train, X1_val, X1_test for the third dataset\n",
    "X1_train_scaled = scaler.transform(X1_train)\n",
    "X1_val_scaled = scaler.transform(X1_val)\n",
    "X1_test_scaled = scaler.transform(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03a712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe5432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    class Sampling(layers.Layer):\n",
    "\n",
    "        def call(self, inputs):\n",
    "            z_mean, z_log_var = inputs\n",
    "            batch = tf.shape(z_mean)[0]\n",
    "            dim = tf.shape(z_mean)[1]\n",
    "            epsilon = tf.random.normal(shape=(batch, dim))\n",
    "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d45ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (332909,)\n",
    "latent_dim = 50\n",
    "batch_size = 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe1a1c7-92c9-4a94-9b3f-332c1a514ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, filters, kernel_size=3, stride=1, l1_reg=0.05, l2_reg=0.05, dropout_rate=0.4):\n",
    "    shortcut = x\n",
    "    x = layers.Dense(filters, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(filters, activation=None, kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Adjust the shortcut if necessary\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Dense(filters, activation=None, kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg))(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "    \n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.ReLU()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0a9659",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    def build_encoder(input_shape, latent_dim, l1_reg=0.05, l2_reg=0.05, dropout_rate=0.4):\n",
    "        encoder_inputs = layers.Input(shape=input_shape)\n",
    "        x = layers.Dense(800, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg))(encoder_inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        \n",
    "        # Add residual blocks\n",
    "        x = residual_block(x, 800, l1_reg=l1_reg, l2_reg=l2_reg, dropout_rate=dropout_rate)\n",
    "        x = residual_block(x, 400, l1_reg=l1_reg, l2_reg=l2_reg, dropout_rate=dropout_rate)\n",
    "        x = residual_block(x, 200, l1_reg=l1_reg, l2_reg=l2_reg, dropout_rate=dropout_rate)\n",
    "        x = residual_block(x, 100, l1_reg=l1_reg, l2_reg=l2_reg, dropout_rate=dropout_rate)\n",
    "        \n",
    "        z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "        z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "        z = Sampling()([z_mean, z_log_var])\n",
    "        encoder = models.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "        return encoder\n",
    "\n",
    "    encoder = build_encoder(input_shape, latent_dim, l1_reg=0.05, l2_reg=0.05, dropout_rate=0.4)\n",
    "    encoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dd0cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    def build_decoder(latent_dim, output_shape, l1_reg=0.05, l2_reg=0.05, dropout_rate=0.4):\n",
    "        latent_inputs = layers.Input(shape=(latent_dim,))\n",
    "        x = layers.Dense(100, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg))(latent_inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        \n",
    "        # Add residual blocks\n",
    "        x = residual_block(x, 100, l1_reg=l1_reg, l2_reg=l2_reg, dropout_rate=dropout_rate)\n",
    "        x = residual_block(x, 200, l1_reg=l1_reg, l2_reg=l2_reg, dropout_rate=dropout_rate)\n",
    "        x = residual_block(x, 400, l1_reg=l1_reg, l2_reg=l2_reg, dropout_rate=dropout_rate)\n",
    "        x = residual_block(x, 800, l1_reg=l1_reg, l2_reg=l2_reg, dropout_rate=dropout_rate)\n",
    "        \n",
    "        decoder_outputs = layers.Dense(output_shape, activation=\"sigmoid\")(x)\n",
    "        decoder = models.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "        return decoder\n",
    "\n",
    "    decoder = build_decoder(latent_dim, 332909, l1_reg=0.05, l2_reg=0.05, dropout_rate=0.4)\n",
    "    decoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d57a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoopmanOperator(tf.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.k_matrix_ut = tf.Variable(\n",
    "            tf.random.uniform((latent_dim * (latent_dim - 1) // 2,), dtype=tf.float32, minval=1e-5, maxval=1.0)\n",
    "        )\n",
    "        self.k_matrix_diag = tf.Variable(\n",
    "            tf.random.uniform((latent_dim,), dtype=tf.float32, minval=1e-5, maxval=1.0)\n",
    "        )\n",
    "\n",
    "    def koopman_operation(self, g):\n",
    "        k_matrix = self._build_koopman_matrix()\n",
    "        g_next = tf.linalg.matmul(g, k_matrix)\n",
    "        scalar_output = tf.reduce_mean(g_next, axis=-1)\n",
    "        return g_next, scalar_output\n",
    "\n",
    "    def get_koopman_matrix(self):\n",
    "        k_matrix = self._build_koopman_matrix()\n",
    "        return k_matrix\n",
    "\n",
    "    def _build_koopman_matrix(self):\n",
    "        k_matrix = tf.zeros((self.latent_dim, self.latent_dim), dtype=tf.float32)\n",
    "        \n",
    "        upper_triangular_indices = tf.linalg.band_part(tf.ones((self.latent_dim, self.latent_dim)), 0, -1)\n",
    "        diag_indices = tf.linalg.diag_part(upper_triangular_indices)\n",
    "        \n",
    "        upper_triangular_values = tf.concat([self.k_matrix_ut, self.k_matrix_diag], axis=0)\n",
    "        k_matrix = tf.linalg.set_diag(k_matrix, self.k_matrix_diag)\n",
    "        \n",
    "        k_matrix += tf.linalg.band_part(\n",
    "            tf.scatter_nd(tf.where(upper_triangular_indices), upper_triangular_values, [self.latent_dim, self.latent_dim]),\n",
    "            0, -1\n",
    "        )\n",
    "        \n",
    "        return k_matrix\n",
    "\n",
    "    def koopman_sparsity_loss(self, l1_factor=0.1):\n",
    "        ut_sparsity_loss = tf.reduce_mean(tf.abs(self.k_matrix_ut))\n",
    "        diag_sparsity_loss = tf.reduce_mean(tf.abs(self.k_matrix_diag))\n",
    "        total_sparsity_loss = l1_factor * (ut_sparsity_loss + diag_sparsity_loss)\n",
    "        return total_sparsity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7730c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    class KoopmanModel(tf.keras.Model):\n",
    "        def __init__(self, koopman_operator):\n",
    "            super(KoopmanModel, self).__init__()\n",
    "            self.koopman_operator = koopman_operator\n",
    "\n",
    "        def call(self, input_present, input_future):\n",
    "            # Define the forward pass using the koopman_operator\n",
    "            g_next_present, daf_present = self.koopman_operator.koopman_operation(input_present)\n",
    "            g_next_future, daf_future = self.koopman_operator.koopman_operation(input_future)\n",
    "            return g_next_present, daf_present, g_next_future, daf_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f116759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class loader(keras.utils.Sequence):\n",
    "    def __init__(self, X, X0, X1, batch_size):\n",
    "        self.X0 = X0\n",
    "        self.X = X\n",
    "        self.X1 = X1\n",
    "        self.num_batches_ts = len(self.X0) // batch_size\n",
    "        self.num_batches_cs = len(self.X) // batch_size\n",
    "        if self.num_batches_ts == self.num_batches_cs:\n",
    "            self.ratio = 1\n",
    "        else:\n",
    "            self.ratio = self.num_batches_cs // self.num_batches_ts\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches_ts\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start_cs = index * self.batch_size * self.ratio\n",
    "        end_cs = start_cs + (self.batch_size * self.ratio)\n",
    "        batch_X_train = self.X[start_cs:end_cs]\n",
    "\n",
    "        # Get a batch of time-series data\n",
    "        start_ts = index * self.batch_size\n",
    "        end_ts = start_ts + self.batch_size\n",
    "        batch_X0_train = self.X0[start_ts:end_ts]\n",
    "        batch_X1_train = self.X1[start_ts:end_ts]\n",
    "\n",
    "        return [batch_X_train, batch_X0_train, batch_X1_train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea7f525",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = loader(X_train_scaled, X0_train_scaled, X1_train_scaled, batch_size)\n",
    "val_loader = loader(X_val_scaled, X0_val_scaled, X1_val_scaled, batch_size)\n",
    "steps_per_epoch = len(X_train_scaled) // batch_size\n",
    "validation_steps = len(X_val_scaled) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a6c7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    class VAE(keras.Model):\n",
    "        def __init__(self, encoder, decoder, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "          \n",
    "\n",
    "        def call(self, inputs):\n",
    "            print(len(inputs))\n",
    "            z_mean_cross, z_log_var_cross, z_cross = self.encoder(inputs[0])\n",
    "            z_mean_present, z_log_var_present, z_present = self.encoder(inputs[1])\n",
    "            z_mean_future, z_log_var_future, z_future = self.encoder(inputs[2])\n",
    "            reconstruction = self.decoder(z_cross)\n",
    "\n",
    "            return reconstruction, z_present, z_future, z_mean_cross, z_log_var_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23a61c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    class MyModel(keras.Model):\n",
    "        def __init__(self, vae, koopman, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.vae = vae\n",
    "            self.koopman = koopman            \n",
    "            self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "            self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "            self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "            self.linear_dynamics_loss_tracker = keras.metrics.Mean(name=\"linear_dynamics_loss\")\n",
    "            self.daf_loss_tracker = keras.metrics.Mean(name=\"daf_loss\")\n",
    "            self.auxiliary_loss_tracker = keras.metrics.Mean(name = \"auxiliary_loss\")\n",
    "            self.koopman_sparsity_loss_tracker = keras.metrics.Mean(name = \"koopman_sparsity_loss\")\n",
    "\n",
    "        @property\n",
    "        def metrics(self):\n",
    "            return [\n",
    "                self.total_loss_tracker,\n",
    "                self.reconstruction_loss_tracker,\n",
    "                self.kl_loss_tracker,\n",
    "                self.linear_dynamics_loss_tracker,\n",
    "                self.daf_loss_tracker,\n",
    "                self.auxiliary_loss_tracker,\n",
    "                self.koopman_sparsity_loss_tracker\n",
    "            ]\n",
    "        \n",
    "        def call(self, inputs):\n",
    "            reconstruction, z_present, z_future, z_mean_cross, z_log_var_cross = self.vae(inputs)\n",
    "            g_next_present, daf_present, g_next_future, daf_future = self.koopman(z_present, z_future)            \n",
    "            decoded_future = self.vae.decoder(g_next_present)\n",
    "            return g_next_present, daf_present, g_next_future, daf_future, reconstruction, decoded_future, z_present, z_future, z_mean_cross, z_log_var_cross\n",
    "        \n",
    "        def train_step(self, data):\n",
    "            data_unpacked = data[0]\n",
    "            input_data_cross, input_data_present, input_data_future = data_unpacked\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                g_next_present, daf_present, g_next_future, daf_future, reconstruction, decoded_future, z_present, z_future, z_mean_cross, z_log_var_cross = self(data_unpacked, training=True)\n",
    "\n",
    "                # Reconstruction Loss\n",
    "                reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.square(input_data_cross - reconstruction), axis=-1))\n",
    "                \n",
    "\n",
    "                # KL Divergence Loss\n",
    "                kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var_cross - tf.square(z_mean_cross) - tf.exp(z_log_var_cross))\n",
    "\n",
    "                # Linear Dynamics Loss\n",
    "                linear_dynamics_loss = tf.reduce_mean(tf.reduce_sum(tf.square(z_future - g_next_present), axis=-1))\n",
    "\n",
    "                # DAF Loss\n",
    "                daf_loss = tf.reduce_mean(tf.reduce_sum(tf.square(daf_present - daf_future), axis=-1))\n",
    "\n",
    "                # Auxiliary Loss\n",
    "                auxiliary_loss = tf.reduce_mean(tf.reduce_sum(tf.square(input_data_future - decoded_future), axis=-1))\n",
    "\n",
    "                # Sparsity Loss for Koopman Operator\n",
    "                koopman_sparsity_loss = self.koopman.koopman_operator.koopman_sparsity_loss()\n",
    "\n",
    "                # Total Loss\n",
    "                total_loss = (\n",
    "                    reconstruction_loss +\n",
    "                    kl_loss +\n",
    "                    linear_dynamics_loss +\n",
    "                    daf_loss +\n",
    "                    auxiliary_loss +\n",
    "                    koopman_sparsity_loss\n",
    "                )\n",
    "\n",
    "            trainable_vars = self.trainable_variables\n",
    "            grads = tape.gradient(total_loss, trainable_vars)\n",
    "            self.optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "\n",
    "            self.total_loss_tracker.update_state(total_loss)\n",
    "            self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "            self.kl_loss_tracker.update_state(kl_loss)\n",
    "            self.linear_dynamics_loss_tracker.update_state(linear_dynamics_loss)\n",
    "            self.daf_loss_tracker.update_state(daf_loss)\n",
    "            self.auxiliary_loss_tracker.update_state(auxiliary_loss)\n",
    "            self.koopman_sparsity_loss_tracker.update_state(koopman_sparsity_loss)\n",
    "\n",
    "            return {\n",
    "                \"total_loss\": self.total_loss_tracker.result(),\n",
    "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "                \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "                \"linear_dynamics_loss\": self.linear_dynamics_loss_tracker.result(),\n",
    "                \"daf_loss\": self.daf_loss_tracker.result(),\n",
    "                \"auxiliary_loss\": self.auxiliary_loss_tracker.result(),\n",
    "                \"koopman_sparsity_loss\": self.koopman_sparsity_loss_tracker.result()\n",
    "            }\n",
    "        \n",
    "        def test_step(self, data):\n",
    "            data_unpacked = data[0]\n",
    "            input_data_cross, input_data_present, input_data_future = data_unpacked\n",
    "            g_next_present, daf_present, g_next_future, daf_future, reconstruction, decoded_future, z_present, z_future, z_mean_cross, z_log_var_cross = self(data_unpacked, training=True)\n",
    "\n",
    "            # Reconstruction Loss\n",
    "            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.square(input_data_cross - reconstruction), axis=-1))\n",
    "            \n",
    "\n",
    "            # KL Divergence Loss\n",
    "            kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var_cross - tf.square(z_mean_cross) - tf.exp(z_log_var_cross))\n",
    "\n",
    "            # Linear Dynamics Loss\n",
    "            linear_dynamics_loss = tf.reduce_mean(tf.reduce_sum(tf.square(z_future - g_next_present), axis=-1))\n",
    "\n",
    "            # DAF Loss\n",
    "            daf_loss = tf.reduce_mean(tf.reduce_sum(tf.square(daf_present - daf_future), axis=-1))\n",
    "\n",
    "            #Auxiliary Loss\n",
    "            auxiliary_loss = tf.reduce_mean(tf.reduce_sum(tf.square(input_data_future - decoded_future), axis=-1))\n",
    "\n",
    "            # Sparsity Loss for Koopman Operator\n",
    "            koopman_sparsity_loss = self.koopman.koopman_operator.koopman_sparsity_loss()\n",
    "\n",
    "            # Total Loss\n",
    "            total_loss = (\n",
    "                reconstruction_loss +\n",
    "                kl_loss +\n",
    "                linear_dynamics_loss +\n",
    "                daf_loss +\n",
    "                auxiliary_loss +\n",
    "                koopman_sparsity_loss\n",
    "            )\n",
    "\n",
    "            self.total_loss_tracker.update_state(total_loss)\n",
    "            self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "            self.kl_loss_tracker.update_state(kl_loss)\n",
    "            self.linear_dynamics_loss_tracker.update_state(linear_dynamics_loss)\n",
    "            self.daf_loss_tracker.update_state(daf_loss)\n",
    "            self.auxiliary_loss_tracker.update_state(auxiliary_loss)\n",
    "            self.koopman_sparsity_loss_tracker.update_state(koopman_sparsity_loss)\n",
    "\n",
    "            return {\n",
    "                \"total_loss\": self.total_loss_tracker.result(),\n",
    "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "                \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "                \"linear_dynamics_loss\": self.linear_dynamics_loss_tracker.result(),\n",
    "                \"daf_loss\": self.daf_loss_tracker.result(),\n",
    "                \"auxiliary_loss\": self.auxiliary_loss_tracker.result(),\n",
    "                \"koopman_sparsity_loss\": self.koopman_sparsity_loss_tracker.result()\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98c49e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6786034",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    import time\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    vae = VAE(encoder, decoder)\n",
    "    koopman_operator = KoopmanOperator(latent_dim)\n",
    "    koopman = KoopmanModel(koopman_operator)   \n",
    "    model = MyModel(vae, koopman)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate = 0.0001, clipvalue=1.0, clipnorm=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2646652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "n_batches = 8\n",
    "\n",
    "filepath = \"C:\\\\Users\\\\G20187729\\\\Desktop\\\\Python_codes_3\\\\Data\\\\Saved_models_20240711_withoutold\\\\saved-model-{epoch:02d}DAF.ckpt\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = filepath, monitor='val_total_loss', verbose=1, save_weights_only=True, save_freq=50*n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61375497",
   "metadata": {},
   "outputs": [],
   "source": [
    "early = EarlyStopping(monitor=\"total_loss\", mode=\"min\", patience=5, restore_best_weights=True)\n",
    "callbacks_list = [early, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb1f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch % 250 == 0 and epoch != 0:\n",
    "        return lr * 0.1  # reduce learning rate by a factor of 10\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "# Create a learning rate scheduler callback\n",
    "lr_scheduler_callback = LearningRateScheduler(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddef2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(filepath.format(epoch=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cab0b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    hist = model.fit(\n",
    "        train_loader,\n",
    "        epochs=1000,\n",
    "#         steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=val_loader,\n",
    "        validation_freq=1,\n",
    "#         validation_steps=validation_steps,\n",
    "        callbacks=[checkpoint]\n",
    "    )\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(f'Training time: {hms_string(elapsed)}')\n",
    "#     print(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ef1422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbd7290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0be086",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    model.load_weights(\"C:\\\\Users\\\\G20187729\\\\Desktop\\\\Python_codes_3\\\\Data\\\\Saved_models_20240711_withoutold\\\\saved-model-300DAF.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b00d5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468675da",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = loader(X_test_scaled, X0_test_scaled, X1_test_scaled, test_batch_size)\n",
    "test_steps = len(X_test_scaled) // test_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67aed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ef5fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74279638",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5104ecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dai_P = pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3289fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dai_F = pred[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853b740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"C:\\\\Users\\\\G20187729\\\\Desktop\\\\Python_codes_3\\\\Data\\\\perftest\\\\dai_P.csv\", dai_P, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b64d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"C:\\\\Users\\\\G20187729\\\\Desktop\\\\Python_codes_3\\\\Data\\\\perftest\\\\dai_F.csv\", dai_F, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb64d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"C:\\\\Users\\\\G20187729\\\\Desktop\\\\Python_codes_3\\\\Data\\\\perftest\\\\y_test.csv\", y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f089730e-36d5-4010-b8e7-ef5c681ffbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames from scalar values\n",
    "df1_dai_P = pd.DataFrame({'dai_P': [dai_P]})\n",
    "df1_dai_F = pd.DataFrame({'dai_F': [dai_F]})\n",
    "\n",
    "# Extract scalar values from DataFrames\n",
    "x = df1_dai_P['dai_P'].values[0]  # Extract the scalar value of dai_P\n",
    "y = df1_dai_F['dai_F'].values[0]  # Extract the scalar value of dai_F\n",
    "\n",
    "# Plotting using matplotlib\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x, y, marker='o', s=100, c='blue', label='Data Points')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('dai_P')\n",
    "plt.ylabel('dai_F')\n",
    "plt.title('Scatter Plot of dai_P vs dai_F')\n",
    "\n",
    "# Displaying legend\n",
    "plt.legend()\n",
    "\n",
    "# Showing plot\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef36879-ca4b-4a3c-8c89-39e256aa4021",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = loader(X_test_scaled, X_test_scaled, X_test_scaled, test_batch_size)\n",
    "test_steps = len(X_test_scaled) // test_batch_size\n",
    "\n",
    "pred = model.predict(test_loader)\n",
    "\n",
    "dai_P = pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902c742f-165f-47ed-a32a-4cda5a13bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_test = y1_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "# Print length to ensure they have the same length\n",
    "print(\"Length of y1_test:\", len(y1_test))\n",
    "print(\"Length of y_test:\", len(y_test))\n",
    "\n",
    "# Convert to DataFrames\n",
    "df_y1 = pd.DataFrame({'y1': y1_test})\n",
    "df_y = pd.DataFrame({'y': y_test})\n",
    "df_dai = pd.DataFrame(dai_P, columns=['DAI'])\n",
    "\n",
    "# Combine dataframes\n",
    "df = pd.concat([df_y1, df_y, df_dai], axis=1)\n",
    "\n",
    "# Ensure 'y1' is categorical if necessary\n",
    "df['y1'] = df['y1'].astype('category')\n",
    "\n",
    "# Check the resulting DataFrames\n",
    "print(\"DataFrame df_y1:\")\n",
    "print(df_y1.head())\n",
    "print(\"DataFrame df_y:\")\n",
    "print(df_y.head())\n",
    "print(\"DataFrame df_dai:\")\n",
    "print(df_dai.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8752ef8-17b0-4bee-bd11-d559f886eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='y', y='DAI', hue='y1', palette='Set1', s=30)\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Scatter Plot of Age vs DAI')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('DAI')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf9798-f06a-4ab4-a796-1a233b244c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_test = y1_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "# Print length to ensure they have the same length\n",
    "print(\"Length of y1_test:\", len(y1_test))\n",
    "print(\"Length of y_test:\", len(y_test))\n",
    "\n",
    "# Convert to DataFrames\n",
    "df_y1 = pd.DataFrame({'y1': y1_test})\n",
    "df_y = pd.DataFrame({'y': y_test})\n",
    "df_dai = pd.DataFrame(dai_P, columns=['DAI'])\n",
    "\n",
    "# Combine dataframes\n",
    "df = pd.concat([df_y1, df_y, df_dai], axis=1)\n",
    "\n",
    "# Ensure 'y1' is categorical if necessary\n",
    "df['y1'] = df['y1'].astype('category')\n",
    "\n",
    "# Define age groups\n",
    "# Check for NaN values and convert 'y' to numeric if necessary\n",
    "df['y'] = pd.to_numeric(df['y'], errors='coerce')\n",
    "\n",
    "# Handle NaN values if any\n",
    "df = df.dropna(subset=['y'])\n",
    "\n",
    "# Bin the ages into intervals of 10 years\n",
    "bins = range(0, int(y_test.max()) + 10, 10)\n",
    "labels = [f\"{i}-{i+9}\" for i in bins[:-1]]\n",
    "df['AgeGroup'] = pd.cut(df['y'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Calculate mean and standard error of DAI for each age group\n",
    "grouped = df.groupby('AgeGroup')['DAI'].agg(['mean', 'sem']).reset_index()\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.pointplot(data=grouped, x='AgeGroup', y='mean', capsize=0.1)\n",
    "plt.errorbar(x=np.arange(len(grouped)), y=grouped['mean'], yerr=grouped['sem'], fmt='o', color='red')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Mean DAI with Standard Error by Age Group')\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Mean DAI')\n",
    "\n",
    "# Rotate x-axis labels if necessary\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ab56c8-1b69-4253-8214-1ead63ac4bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'y1' and 'y_test' have the same length and reset indices\n",
    "y1_test = y1_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "# Print length to ensure they have the same length\n",
    "print(\"Length of y1_test:\", len(y1_test))\n",
    "print(\"Length of y_test:\", len(y_test))\n",
    "\n",
    "# Convert to DataFrames\n",
    "df_y1 = pd.DataFrame({'y1': y1_test})\n",
    "df_y = pd.DataFrame({'y': y_test})\n",
    "df_dai = pd.DataFrame(dai_P, columns=['DAI'])\n",
    "\n",
    "# Combine dataframes\n",
    "df = pd.concat([df_y1, df_y, df_dai], axis=1)\n",
    "\n",
    "# Ensure 'y1' is categorical if necessary\n",
    "df['y1'] = df['y1'].astype('category')\n",
    "\n",
    "# Define age groups\n",
    "# Check for NaN values and convert 'y' to numeric if necessary\n",
    "df['y'] = pd.to_numeric(df['y'], errors='coerce')\n",
    "\n",
    "# Handle NaN values if any\n",
    "df = df.dropna(subset=['y'])\n",
    "\n",
    "# Bin the ages into intervals of 10 years\n",
    "bins = range(0, int(y_test.max()) + 10, 10)\n",
    "labels = [f\"{i}-{i+9}\" for i in bins[:-1]]\n",
    "df['AgeGroup'] = pd.cut(df['y'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Calculate mean and standard error of DAI for each age group and cohort\n",
    "grouped = df.groupby(['AgeGroup', 'y1'])['DAI'].agg(['mean', 'sem']).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "# Add error bars manually\n",
    "for i, age_group in enumerate(labels):\n",
    "    for j, cohort in enumerate(df['y1'].cat.categories):\n",
    "        subset = grouped[(grouped['AgeGroup'] == age_group) & (grouped['y1'] == cohort)]\n",
    "        if not subset.empty:\n",
    "            plt.errorbar(x=i + j*0.2, y=subset['mean'].values[0], yerr=subset['sem'].values[0], fmt='o', color=sns.color_palette('Set1')[j])\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Mean DAI with Standard Error by Age Group and Cohort')\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Mean DAI')\n",
    "\n",
    "# Rotate x-axis labels if necessary\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11bd263-a826-4aa4-8181-65da4becadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dataframes\n",
    "df = pd.concat([df_y, df_dai], axis=1)\n",
    "\n",
    "# Define age groups\n",
    "# Check for NaN values and convert 'y' to numeric if necessary\n",
    "df['y'] = pd.to_numeric(df['y'], errors='coerce')\n",
    "\n",
    "# Handle NaN values if any\n",
    "df = df.dropna(subset=['y'])\n",
    "\n",
    "# Define bins and labels\n",
    "bins = range(0, int(max(df['y'])) + 10, 10)  # Convert max(df['y']) to int\n",
    "labels = [f'{i}-{i+9}' for i in bins[:-1]]  # Labels for the age groups\n",
    "df['age_group'] = pd.cut(df['y'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Calculate statistics\n",
    "grouped = df.groupby('age_group')['DAI'].agg(['mean', 'std', 'count']).reset_index()\n",
    "\n",
    "# Plotting\n",
    "plt.errorbar(grouped['age_group'], grouped['mean'], yerr=grouped['std'], fmt='o', capsize=5)\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Mean DAI')\n",
    "plt.title('Mean DAI with Standard Deviation by Age Group')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e901ea-21d6-4b1a-bc0b-c4cf9d06c574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dataframes\n",
    "df = pd.concat([df_y, df_dai], axis=1)\n",
    "\n",
    "# Define age groups\n",
    "# Check for NaN values and convert 'y' to numeric if necessary\n",
    "df['y'] = pd.to_numeric(df['y'], errors='coerce')\n",
    "\n",
    "# Handle NaN values if any\n",
    "df = df.dropna(subset=['y'])\n",
    "\n",
    "# Define bins and labels\n",
    "bins = range(0, int(max(df['y'])) + 10, 10)  # Convert max(df['y']) to int\n",
    "labels = [f'{i}-{i+9}' for i in bins[:-1]]  # Labels for the age groups\n",
    "df['age_group'] = pd.cut(df['y'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Calculate statistics\n",
    "grouped = df.groupby('age_group')['DAI'].agg(['mean', 'std', 'count']).reset_index()\n",
    "\n",
    "# Calculate Standard Error (SE)\n",
    "grouped['SE'] = grouped['std'] / np.sqrt(grouped['count'])\n",
    "\n",
    "# Plotting\n",
    "plt.errorbar(grouped['age_group'], grouped['mean'], yerr=grouped['SE'], fmt='o', capsize=5)\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Mean DAI')\n",
    "plt.title('Mean DAI with Standard Error by Age Group')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b6c5ba-62ee-4970-8ddf-10bab21150e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "df = pd.concat([df_y, df_dai], axis=1)\n",
    "\n",
    "# Check for NaN values and convert 'y' to numeric if necessary\n",
    "df['y'] = pd.to_numeric(df['y'], errors='coerce')\n",
    "\n",
    "# Handle NaN values if any\n",
    "df = df.dropna(subset=['y', 'DAI'])\n",
    "\n",
    "# Remove inf values\n",
    "df = df[np.isfinite(df['y'])]\n",
    "df = df[np.isfinite(df['DAI'])]\n",
    "\n",
    "# Extract age and DAI values\n",
    "age = df['y'].values\n",
    "dai = df['DAI'].values\n",
    "\n",
    "# Ensure there are no NaN or inf values\n",
    "if np.any(np.isnan(age)) or np.any(np.isnan(dai)):\n",
    "    raise ValueError(\"Data contains NaN values\")\n",
    "if np.any(np.isinf(age)) or np.any(np.isinf(dai)):\n",
    "    raise ValueError(\"Data contains infinite values\")\n",
    "\n",
    "# Normalize data\n",
    "age_norm = (age - np.min(age)) / (np.max(age) - np.min(age))\n",
    "dai_norm = (dai - np.min(dai)) / (np.max(dai) - np.min(dai))\n",
    "\n",
    "# Define the Gompertz-Makeham function\n",
    "def gompertz_makeham(x, A, B, C):\n",
    "    return A + B * np.exp(C * x)\n",
    "\n",
    "# Fit the model to the data with different initial guesses and bounds\n",
    "initial_guess = [0.1, 0.1, 0.01]\n",
    "bounds = (0, [10., 10., 1.])\n",
    "\n",
    "popt, pcov = curve_fit(gompertz_makeham, age_norm, dai_norm, p0=initial_guess, bounds=bounds)\n",
    "A, B, C = popt\n",
    "\n",
    "# Generate fitted values for plotting\n",
    "fitted_dai_norm = gompertz_makeham(age_norm, A, B, C)\n",
    "\n",
    "# Denormalize the fitted values\n",
    "fitted_dai = fitted_dai_norm * (np.max(dai) - np.min(dai)) + np.min(dai)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(age, dai, label='Observed DAI', color='blue')\n",
    "plt.plot(age, fitted_dai, label=f'Gompertz-Makeham fit (A={A:.2f}, B={B:.2f}, C={C:.2f})', color='red')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('DAI')\n",
    "plt.title('DAI with Gompertz-Makeham Fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1bb041-c790-49c0-9db9-797122519b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dataframes and prepare data\n",
    "df = pd.concat([df_y, df_dai], axis=1)\n",
    "\n",
    "# Convert 'y' to numeric and handle NaN values\n",
    "df['y'] = pd.to_numeric(df['y'], errors='coerce')\n",
    "df = df.dropna(subset=['y', 'DAI'])\n",
    "df = df[np.isfinite(df['y'])]\n",
    "df = df[np.isfinite(df['DAI'])]\n",
    "\n",
    "# Extract age and DAI values\n",
    "age = df['y'].values\n",
    "dai = df['DAI'].values\n",
    "\n",
    "# Ensure there are no NaN or inf values\n",
    "if np.any(np.isnan(age)) or np.any(np.isnan(dai)):\n",
    "    raise ValueError(\"Data contains NaN values\")\n",
    "if np.any(np.isinf(age)) or np.any(np.isinf(dai)):\n",
    "    raise ValueError(\"Data contains infinite values\")\n",
    "\n",
    "# Group age into 10-year intervals\n",
    "df['age_group'] = (df['y'] // 10) * 10\n",
    "grouped = df.groupby('age_group').agg({'DAI': 'mean'}).reset_index()\n",
    "\n",
    "# Extract grouped data\n",
    "age_grouped = grouped['age_group'].values\n",
    "dai_grouped = grouped['DAI'].values\n",
    "\n",
    "# Normalize data\n",
    "age_norm = (age - np.min(age)) / (np.max(age) - np.min(age))\n",
    "dai_norm = (dai - np.min(dai)) / (np.max(dai) - np.min(dai))\n",
    "\n",
    "# Normalize grouped data\n",
    "age_grouped_norm = (age_grouped - np.min(age)) / (np.max(age) - np.min(age))\n",
    "dai_grouped_norm = (dai_grouped - np.min(dai)) / (np.max(dai) - np.min(dai))\n",
    "\n",
    "# Define the Gompertz-Makeham function\n",
    "def gompertz_makeham(x, A, B, C):\n",
    "    return A + B * np.exp(C * x)\n",
    "\n",
    "# Fit the model to the grouped data with different initial guesses and bounds\n",
    "initial_guess = [0.1, 0.1, 0.01]\n",
    "bounds = (0, [10., 10., 1.])\n",
    "\n",
    "popt, pcov = curve_fit(gompertz_makeham, age_grouped_norm, dai_grouped_norm, p0=initial_guess, bounds=bounds)\n",
    "A, B, C = popt\n",
    "\n",
    "# Generate fitted values for plotting\n",
    "fitted_dai_grouped_norm = gompertz_makeham(age_grouped_norm, A, B, C)\n",
    "\n",
    "# Denormalize the fitted values\n",
    "fitted_dai_grouped = fitted_dai_grouped_norm * (np.max(dai) - np.min(dai)) + np.min(dai)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(age_grouped, dai_grouped, label='Mean DAI per Age Group', color='blue')\n",
    "plt.plot(age_grouped, fitted_dai_grouped, label=f'Gompertz-Makeham fit ', color='red')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('DAI')\n",
    "plt.title('Mean DAI by Age Group with Gompertz-Makeham Fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246fae24-a956-47df-89a3-7595454fbec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dataframes and prepare data\n",
    "# Concatenate dataframes and prepare data\n",
    "df = pd.concat([df_y, df_dai], axis=1)\n",
    "\n",
    "# Convert 'y' to numeric and handle NaN values\n",
    "df['y'] = pd.to_numeric(df['y'], errors='coerce')\n",
    "df = df.dropna(subset=['y', 'DAI'])\n",
    "df = df[np.isfinite(df['y'])]\n",
    "df = df[np.isfinite(df['DAI'])]\n",
    "\n",
    "# Extract age and DAI values\n",
    "age = df['y'].values\n",
    "dai = df['DAI'].values\n",
    "\n",
    "# Ensure there are no NaN or inf values\n",
    "if np.any(np.isnan(age)) or np.any(np.isnan(dai)):\n",
    "    raise ValueError(\"Data contains NaN values\")\n",
    "if np.any(np.isinf(age)) or np.any(np.isinf(dai)):\n",
    "    raise ValueError(\"Data contains infinite values\")\n",
    "\n",
    "# Group age into 10-year intervals\n",
    "df['age_group'] = (df['y'] // 10) * 10\n",
    "grouped = df.groupby('age_group').agg({'DAI': 'mean'}).reset_index()\n",
    "\n",
    "# Extract grouped data\n",
    "age_grouped = grouped['age_group'].values\n",
    "dai_grouped = grouped['DAI'].values\n",
    "\n",
    "# Normalize data\n",
    "age_min, age_max = np.min(age), np.max(age)\n",
    "dai_min, dai_max = np.min(dai), np.max(dai)\n",
    "age_grouped_norm = (age_grouped - age_min) / (age_max - age_min)\n",
    "dai_grouped_norm = (dai_grouped - dai_min) / (dai_max - dai_min)\n",
    "\n",
    "# Define the exponential function\n",
    "def exponential(x, a, b):\n",
    "    return a * np.exp(b * x)\n",
    "\n",
    "# Fit exponential model\n",
    "popt_exp, _ = curve_fit(exponential, age_grouped_norm, dai_grouped_norm, p0=[1, 0.01])\n",
    "a_exp, b_exp = popt_exp\n",
    "\n",
    "# Generate fitted values for the normalized data\n",
    "fitted_dai_grouped_norm = exponential(age_grouped_norm, *popt_exp)\n",
    "\n",
    "# Denormalize the fitted values\n",
    "fitted_dai_grouped = fitted_dai_grouped_norm * (dai_max - dai_min) + dai_min\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(age_grouped, dai_grouped, label='Mean DAI per Age Group', color='blue')\n",
    "plt.plot(age_grouped, fitted_dai_grouped, label=f'Exponential fit (a={a_exp:.2f}, b={b_exp:.2f})', color='red')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('DAI')\n",
    "plt.title('Mean DAI by Age Group with Exponential Fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3616eafb-1683-4538-a559-8b9835edef4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e80066-cafb-4e4e-8dc8-ccbb0836aa41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6614cf-aa28-4d21-b43d-0bf6f103e67b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7546fd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_r_squared(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculate R-squared (coefficient of determination) for actual and predicted values.\n",
    "\n",
    "    :param actual: list or numpy array of actual values\n",
    "    :param predicted: list or numpy array of predicted values\n",
    "    :return: R-squared value\n",
    "    \"\"\"\n",
    "    # Convert lists to numpy arrays for easier computation\n",
    "    actual = np.array(actual)\n",
    "    predicted = np.array(predicted)\n",
    "    \n",
    "    # Calculate mean of actual values\n",
    "    mean_actual = np.mean(actual)\n",
    "    \n",
    "    # Calculate total sum of squares\n",
    "    total_sum_squares = np.sum((actual - mean_actual) ** 2)\n",
    "    \n",
    "    # Calculate residual sum of squares\n",
    "    residual_sum_squares = np.sum((actual - predicted) ** 2)\n",
    "    \n",
    "    # Calculate R-squared\n",
    "    r_squared = 1 - (residual_sum_squares / total_sum_squares)\n",
    "    \n",
    "    return r_squared\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65878e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data = X_test_scaled\n",
    "predicted_data = pred[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92026c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = min(len(actual_data), len(predicted_data))\n",
    "actual_data = actual_data[:min_length]\n",
    "predicted_data = predicted_data[:min_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94911289",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_squared_value = calculate_r_squared(actual_data, predicted_data)\n",
    "print(\"R-squared:\", r_squared_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e49b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculate Mean Squared Error (MSE) for actual and predicted values.\n",
    "\n",
    "    :param actual: list or numpy array of actual values\n",
    "    :param predicted: list or numpy array of predicted values\n",
    "    :return: Mean Squared Error (MSE) value\n",
    "    \"\"\"\n",
    "    # Convert lists to numpy arrays for easier computation\n",
    "    actual = np.array(actual)\n",
    "    predicted = np.array(predicted)\n",
    "    \n",
    "    # Calculate squared errors\n",
    "    squared_errors = (actual - predicted) ** 2\n",
    "    \n",
    "    # Calculate Mean Squared Error (MSE)\n",
    "    mse = np.mean(squared_errors)\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b08eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_value = calculate_mse(actual_data, predicted_data)\n",
    "print(\"Mean Squared Error (MSE):\", mse_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rmse(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculate Root Mean Squared Error (RMSE) for actual and predicted values.\n",
    "\n",
    "    :param actual: list or numpy array of actual values\n",
    "    :param predicted: list or numpy array of predicted values\n",
    "    :return: Root Mean Squared Error (RMSE) value\n",
    "    \"\"\"\n",
    "    # Calculate MSE\n",
    "    mse = calculate_mse(actual, predicted)\n",
    "    \n",
    "    # Calculate RMSE by taking square root of MSE\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f243a314",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = calculate_rmse(actual_data, predicted_data)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7bdefc-8ec6-4d48-88c7-82a34e1dc9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b53ddc-4689-4907-9d1a-dfb24c04a48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddc1003-8f6f-422f-a967-5605fd1595f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b4afd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb0c94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_latent_representations_from_loader(loader):\n",
    "    latent_representations_cross = []\n",
    "    latent_representations_present = []\n",
    "    latent_representations_future = []\n",
    "    for batch_data in loader:\n",
    "        input_data_cross, input_data_present, input_data_future = batch_data  \n",
    "        _, _, z_latent_cross = model.vae.encoder(input_data_cross)\n",
    "        _, _, z_latent_present = model.vae.encoder(input_data_present)\n",
    "        _, _, z_latent_future = model.vae.encoder(input_data_future)\n",
    "        latent_representations_cross.append(z_latent_cross.numpy())\n",
    "        latent_representations_present.append(z_latent_present.numpy())\n",
    "        latent_representations_future.append(z_latent_future.numpy())\n",
    "    latent_representations_cross = np.concatenate(latent_representations_cross, axis=0)\n",
    "    latent_representations_present = np.concatenate(latent_representations_present, axis=0)\n",
    "    latent_representations_future = np.concatenate(latent_representations_future, axis=0)\n",
    "    return latent_representations_cross, latent_representations_present, latent_representations_future\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efbb4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    X_train_latent_cross, X_train_latent_present, X_train_latent_future = extract_latent_representations_from_loader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278e55d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cross = X_train_latent_cross.shape[0]\n",
    "num_present = X_train_latent_present.shape[0]\n",
    "num_future = X_train_latent_future.shape[0]\n",
    "\n",
    "# Concatenate latent representations of all three datasets\n",
    "X_all_latent = np.concatenate((X_train_latent_cross, X_train_latent_present, X_train_latent_future), axis=0)\n",
    "\n",
    "# Apply t-SNE to reduce dimensionality\n",
    "tsne = TSNE(n_components=2, perplexity=1000, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_all_latent)\n",
    "\n",
    "# Visualize the t-SNE embeddings with different colors for each dataset\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_tsne[:num_cross, 0], X_tsne[:num_cross, 1], color='blue', label='Cross Dataset', s=10)  # Scatter plot for cross dataset\n",
    "plt.scatter(X_tsne[num_cross:num_cross+num_present, 0], X_tsne[num_cross:num_cross+num_present, 1], color='red', label='Present Dataset', s=10)  # Scatter plot for present dataset\n",
    "plt.scatter(X_tsne[num_cross+num_present:, 0], X_tsne[num_cross+num_present:, 1], color='green', label='Future Dataset', s=10)  # Scatter plot for future dataset\n",
    "plt.title('t-SNE Visualization of Latent Representations')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803186b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c303f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    Z_mean, Z_log_var, Z = vae.encoder(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8013a96c-f369-45e3-af52-f23fe2e75862",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = tf.reduce_mean(Z, axis=0)\n",
    "stddev = tf.math.reduce_std(Z, axis=0)\n",
    "standardized = (Z - mean) / stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae78ade-a3a5-4e81-8ab5-35e317f4483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(standardized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc1859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a07ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_tsne = TSNE(n_components=2, init='random', perplexity=100).fit_transform(standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625bfea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"C:\\\\Users\\\\G20187729\\\\Desktop\\\\Python_codes_3\\\\Data\\\\tsne.csv\", latent_tsne, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b85ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df = pd.DataFrame(latent_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15638af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd9766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = pd.concat([y_train.reset_index(drop=True), tsne_df.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d29c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne.columns = ['Age', 'tsne1', 'tsne2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d465503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = tsne.sort_values(by = 'Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c1a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = tsne.dropna(subset=['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5853ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_copy = tsne.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ba6b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping samples by every 10 years\n",
    "tsne_copy['Age_group'] = pd.cut(tsne['Age'], bins=range(0,101,10), right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50304e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6281c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = cm.tab10(np.linspace(0, 1, len(tsne_copy['Age_group'].unique()))) #colors based on age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7622e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (key, group) in enumerate(tsne_copy.groupby('Age_group', observed=False)):\n",
    "    plt.scatter(group['tsne1'], group['tsne2'], label=key, color=colors[i], s=8)\n",
    "plt.xlabel('t_SNE1')\n",
    "plt.ylabel('t_SNE2')\n",
    "plt.legend(title='Age Group', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08484b86-3174-46c3-a2c5-945baf4235b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_mean_t, Z_log_var_t, Z_t = vae.encoder(X0_test_scaled)\n",
    "Z_mean_t1, Z_log_var_t1, Z_t1 = vae.encoder(X1_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af1beae-deb7-4638-994d-3de4e3118ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_tsne1 = TSNE(n_components=2, init='random', perplexity=50, random_state=10).fit_transform(Z_t)\n",
    "latent_tsne2 = TSNE(n_components=2, init='random', perplexity=50, random_state=10).fit_transform(Z_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73be7088-f9d8-45c7-9d49-d1a6876f1dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.scatter(latent_tsne1[:, 0], latent_tsne1[:, 1], label='t')\n",
    "plt.scatter(latent_tsne2[:, 0], latent_tsne2[:, 1], label='t+1')\n",
    "plt.title('t-SNE Visualization')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9c6c4e-3bf4-4557-ad4f-7528912e2dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Concatenate the data along with time labels\n",
    "X_concat = np.concatenate((X0_test_scaled, X1_test_scaled), axis=0)\n",
    "time_labels = np.concatenate((np.zeros(X0_test_scaled.shape[0]), np.ones(X1_test_scaled.shape[0])))\n",
    "\n",
    "# Perform t-SNE on the concatenated data\n",
    "latent_tsne_concat = TSNE(n_components=2, init='random', perplexity=50, random_state=10).fit_transform(X_concat)\n",
    "\n",
    "# Split the t-SNE results back into t and t+1\n",
    "latent_tsne_t = latent_tsne_concat[:X0_test_scaled.shape[0]]\n",
    "latent_tsne_t1 = latent_tsne_concat[X1_test_scaled.shape[0]:]\n",
    "\n",
    "# Plotting\n",
    "plt.scatter(latent_tsne_t[:, 0], latent_tsne_t[:, 1], label='t')\n",
    "plt.scatter(latent_tsne_t1[:, 0], latent_tsne_t1[:, 1], label='t+1')\n",
    "plt.title('t-SNE Visualization of Concatenated Data')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcaf691-6f10-46dd-91ba-d854625f3724",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Concatenate the data along with time labels\n",
    "X_concat = np.concatenate((X0_train_scaled, X1_train_scaled), axis=0)\n",
    "time_labels = np.concatenate((np.zeros(X0_train_scaled.shape[0]), np.ones(X1_train_scaled.shape[0])))\n",
    "\n",
    "# Perform t-SNE on the concatenated data\n",
    "latent_tsne_concat = TSNE(n_components=2, init='random', perplexity=500, random_state=10).fit_transform(X_concat)\n",
    "\n",
    "# Split the t-SNE results back into t and t+1\n",
    "latent_tsne_t = latent_tsne_concat[:X0_train_scaled.shape[0]]\n",
    "latent_tsne_t1 = latent_tsne_concat[X1_train_scaled.shape[0]:]\n",
    "\n",
    "# Plotting\n",
    "plt.scatter(latent_tsne_t[:, 0], latent_tsne_t[:, 1], label='t')\n",
    "plt.scatter(latent_tsne_t1[:, 0], latent_tsne_t1[:, 1], label='t+1')\n",
    "plt.title('t-SNE Visualization of Concatenated Data')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c03826-b5e1-45d7-9ead-588bfd542ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_mean_t, Z_log_var_t, Z_t = vae.encoder(X0_train_scaled)\n",
    "Z_mean_t1, Z_log_var_t1, Z_t1 = vae.encoder(X1_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c03c6a0-792e-4822-b4c4-474423bf82e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_tsne1 = TSNE(n_components=2, init='random', perplexity=400, random_state=0).fit_transform(Z_t)\n",
    "latent_tsne2 = TSNE(n_components=2, init='random', perplexity=400, random_state=0).fit_transform(Z_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a759f1-3d16-49bb-97e6-1f0c9b9adcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.scatter(latent_tsne1[:, 0], latent_tsne1[:, 1], label='t')\n",
    "plt.scatter(latent_tsne2[:, 0], latent_tsne2[:, 1], label='t+1')\n",
    "plt.title('t-SNE Visualization')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6737a-9d3e-4c31-862b-337a59955629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56be5b0e-8aaa-4c9c-ae19-066607e2bb6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612183fa-f0ef-4360-a416-a5498a05da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_mean, Z_log_var, Z = vae.encoder(X_test_scaled)\n",
    "Z_mean_t, Z_log_var_t, Z_t = vae.encoder(X0_test_scaled)\n",
    "Z_mean_t1, Z_log_var_t1, Z_t1 = vae.encoder(X1_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2b7b60-d353-4f0e-8dac-ae40668b59ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_tsne = TSNE(n_components=2, init='random', perplexity=500, random_state=10).fit_transform(Z)\n",
    "latent_tsne1 = TSNE(n_components=2, init='random', perplexity=50, random_state=10).fit_transform(Z_t)\n",
    "latent_tsne2 = TSNE(n_components=2, init='random', perplexity=50, random_state=10).fit_transform(Z_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e07c2d-112b-4834-b0b4-05a1877f4378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1], label='c')\n",
    "plt.scatter(latent_tsne1[:, 0], latent_tsne1[:, 1], label='t')\n",
    "plt.scatter(latent_tsne2[:, 0], latent_tsne2[:, 1], label='t+1')\n",
    "plt.title('t-SNE Visualization')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66161593-2e76-4e9b-a2a9-8a1a7a22720b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c288c2-ffce-4aa0-85d5-83782966d622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57c571e-6530-4727-921b-b6dff2dbd6cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b538f-a098-4ff9-b160-d4d7eed28b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdefc5bd-90fe-4ea4-94d6-635f775b080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_tsne1 = TSNE(n_components=2, init='random', perplexity=30, random_state=10).fit_transform(X0_test_scaled)\n",
    "latent_tsne2 = TSNE(n_components=2, init='random', perplexity=30, random_state=10).fit_transform(X1_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4f53c8-7bcc-4770-b238-9008b30d0e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.scatter(latent_tsne1[:, 0], latent_tsne1[:, 1], label='t')\n",
    "plt.scatter(latent_tsne2[:, 0], latent_tsne2[:, 1], label='t+1')\n",
    "plt.title('t-SNE Visualization')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ff153-4ec5-4e12-8200-2bb6a86871a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591d145-1e70-4811-a937-9e82031db78e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da06562b-2582-43ac-afcd-0ee9f7857a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6449a60-6b01-4ed9-b08a-5f6e078005e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0258188-9824-4448-9892-610e3023ebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_tsne1 = TSNE(n_components=3, init='random', perplexity=50, random_state=42).fit_transform(Z_t)\n",
    "latent_tsne2 = TSNE(n_components=3, init='random', perplexity=50, random_state=42).fit_transform(Z_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e9552e-ab9a-4d56-8cfa-79e6a91c19e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "# Plotting\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Scatter plot for dataset 1\n",
    "ax.scatter(latent_tsne1[:, 0], latent_tsne1[:, 1], latent_tsne1[:, 2], c='b', label='t')\n",
    "\n",
    "# Scatter plot for dataset 2\n",
    "ax.scatter(latent_tsne2[:, 0], latent_tsne2[:, 1], latent_tsne2[:, 2], c='r', label='t+1')\n",
    "\n",
    "ax.set_title('t-SNE 3D Visualization')\n",
    "ax.set_xlabel('t-SNE Component 1')\n",
    "ax.set_ylabel('t-SNE Component 2')\n",
    "ax.set_zlabel('t-SNE Component 3')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c88247-6b3e-4eba-98c1-a8b3368d82c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4f6853-686a-4bee-8e76-e5bfa4c84459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Perform PCA on each dataset\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "latent_pca_1 = pca.fit_transform(Z_t)\n",
    "latent_pca_2 = pca.fit_transform(Z_t1)\n",
    "\n",
    "# Plotting\n",
    "plt.scatter(latent_pca_1[:, 0], latent_pca_1[:, 1], label='Dataset 1')\n",
    "plt.scatter(latent_pca_2[:, 0], latent_pca_2[:, 1], label='Dataset 2')\n",
    "plt.title('PCA Visualization')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a49c91-dfea-48b2-a3b5-57bce62ea66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Importing 3D plotting tools\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform PCA on each dataset\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "latent_pca_1 = pca.fit_transform(Z_t)\n",
    "latent_pca_2 = pca.fit_transform(Z_t1)\n",
    "\n",
    "# Plotting\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Scatter plot for dataset 1\n",
    "ax.scatter(latent_pca_1[:, 0], latent_pca_1[:, 1], latent_pca_1[:, 2], label='Dataset 1')\n",
    "\n",
    "# Scatter plot for dataset 2\n",
    "ax.scatter(latent_pca_2[:, 0], latent_pca_2[:, 1], latent_pca_2[:, 2], label='Dataset 2')\n",
    "\n",
    "ax.set_title('PCA 3D Visualization')\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_zlabel('PCA Component 3')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0937ca1-9ba0-441a-bcb1-4ecf748d54eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcf9cbd-5333-434e-874d-60a923c52009",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = np.concatenate([X0_train_scaled, X1_train_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd623ebc-d7e4-4598-a8a3-7e185b1b3d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "combined_standardized = (combined - combined.mean(axis=0)) / combined.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be14fc41-2b97-4c69-9901-23586720c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Importing 3D plotting tools\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(combined)\n",
    "\n",
    "# Plot PCA results\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(pca_result[:len(Z_t), 0], pca_result[:len(Z_t), 1], c='b', label='Timepoint 1')\n",
    "plt.scatter(pca_result[len(Z_t):, 0], pca_result[len(Z_t):, 1], c='r', label='Timepoint 2')\n",
    "plt.title('PCA')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef40d9d-2393-47b7-b9cc-ac616c997a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_result = TSNE(n_components=3, init='random', perplexity=300, random_state=0).fit_transform(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88a2699-391c-4bb1-b388-a05ea5ec9e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(tsne_result[:len(X0_train_scaled), 0], tsne_result[:len(X1_train_scaled), 1], c='b', label='t')\n",
    "plt.scatter(tsne_result[len(X0_train_scaled):, 0], tsne_result[len(X1_train_scaled):, 1], c='r', label='t+1')\n",
    "plt.title('t-SNE Visualization')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6188ea8b-8e41-42e6-bff1-33c73753f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_mean_t, Z_log_var_t, Z_t = vae.encoder(X0_train_scaled)\n",
    "Z_mean_t1, Z_log_var_t1, Z_t1 = vae.encoder(X1_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4ed0ba-5f1b-41a5-865c-7d156bd9aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = np.concatenate([Z_t, Z_t1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f7361-0997-4679-a58c-aa0b9e0abc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "combined_standardized = (combined - combined.mean(axis=0)) / combined.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ad122a-3e91-49a5-872e-c27808083517",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_result = TSNE(n_components=2, init='random', perplexity=350, random_state=42).fit_transform(combined_standardized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203a699-ba72-4674-b8ce-bb4397328c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(tsne_result[:len(Z_t), 0], tsne_result[:len(Z_t1), 1], c='b', label='t')\n",
    "plt.scatter(tsne_result[len(Z_t):, 0], tsne_result[len(Z_t1):, 1], c='r', label='t+1')\n",
    "plt.title('t-SNE Visualization')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcb979a-d66c-4625-a3ae-bd6df5fb015a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b0664-2761-455a-8846-24227097608f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f29853-3432-48dc-ac92-2781d667eea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b53124-de1b-4ae1-8bbf-ca0b3755a291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fd7ea2-2c48-47a7-997d-f0d239a9b3d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bd9b74-65d6-4cc2-b36f-ac67b90666b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24289017",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78dd0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "threeD_latent_tsne = TSNE(n_components=3, init='random', perplexity=200).fit_transform(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bba4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"C:\\\\Users\\\\G20187729\\\\Desktop\\\\Python_codes_3\\\\Data\\\\3Dtsne.csv\", latent_tsne, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3805a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df = pd.DataFrame(threeD_latent_tsne)\n",
    "y_test = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5ba3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = pd.concat([y_test.reset_index(drop=True), tsne_df.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5d9bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne.columns = ['Age', 'tsne1', 'tsne2', 'tsne3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e9021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = tsne.sort_values(by = 'Age')\n",
    "tsne = tsne.dropna(subset=['Age'])\n",
    "tsne_copy = tsne.copy()\n",
    "#Grouping samples by every 10 years\n",
    "tsne_copy['Age_group'] = pd.cut(tsne['Age'], bins=range(0,101,30), right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760f6c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acca9461",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = []\n",
    "for key, group in tsne_copy.groupby('Age_group', observed=False):\n",
    "    trace = go.Scatter3d(\n",
    "        x=group['tsne1'],\n",
    "        y=group['tsne2'],\n",
    "        z=group['tsne3'],  # Assuming you have a 'tsne3' column for the third dimension\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=6,\n",
    "            opacity=0.8\n",
    "        ),\n",
    "        name=str(key)\n",
    "    )\n",
    "    traces.append(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38889912",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = go.Layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(title='tsne1'),\n",
    "        yaxis=dict(title='tsne2'),\n",
    "        zaxis=dict(title='tsne3')\n",
    "    ),\n",
    "    legend=dict(\n",
    "        x=0,\n",
    "        y=1.0,\n",
    "        bgcolor='rgba(255,255,255,0)',\n",
    "        bordercolor='rgba(255,255,255,0)',\n",
    "        itemwidth=50\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d26774",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=traces, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2695d319",
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.write_html(fig, 'C:\\\\Users\\\\G20187729\\\\Desktop\\\\Python_codes_3\\\\Data\\\\Koopman best r2_78\\\\Next 280 epochs\\\\3d_tsne_plot.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26039a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39655bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate pairwise distances between latent vectors\n",
    "pairwise_distances = np.linalg.norm(Z[:, None] - Z, axis=-1)\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(pairwise_distances, cmap='viridis', square=True)\n",
    "plt.title('Pairwise Distance Heatmap of Autoencoder Latent Space')\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('Data Points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5717c70b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365813e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    Z_mean, Z_log_var, Z = vae.encoder(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d974fe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87033ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def correlation_analysis(array1, array2, threshold=0.001):\n",
    "    correlations = np.zeros((array1.shape[1], array2.shape[1]))\n",
    "    p_values = np.zeros((array1.shape[1], array2.shape[1]))\n",
    "\n",
    "    for i in range(array1.shape[1]):\n",
    "        for j in range(array2.shape[1]):\n",
    "            correlation, p_value = pearson_correlation(array1[:, i], array2[:, j])\n",
    "            correlations[i, j] = correlation\n",
    "            p_values[i, j] = p_value\n",
    "\n",
    "    # Rank features based on significance (lower p-value means higher significance)\n",
    "    sorted_indices = np.argsort(p_values, axis=None)\n",
    "    ranked_indices = np.unravel_index(sorted_indices, p_values.shape)\n",
    "\n",
    "    # Filter features based on significance threshold\n",
    "    significant_features = [(i, j) for i, j in zip(ranked_indices[0], ranked_indices[1]) if p_values[i, j] < threshold]\n",
    "\n",
    "    return correlations, p_values, significant_features\n",
    "\n",
    "def pearson_correlation(x, y):.\n",
    "    correlation, p_value = stats.pearsonr(x, y)\n",
    "    return correlation, p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6593bec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations, p_values, significant_features = correlation_analysis(Z, X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b83185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to CSV file\n",
    "correlations_df = pd.DataFrame(correlations)\n",
    "p_values_df = pd.DataFrame(p_values)\n",
    "significant_features_df = pd.DataFrame(significant_features, columns=[f\"Feature{i+1}\" for i in range(len(significant_features[0]))])\n",
    "\n",
    "correlations_df.to_csv(\"C:\\\\Users\\\\G20187729\\\\Desktop\\\\Python_codes_3\\\\Data\\\\correlations.csv\", index=False)\n",
    "p_values_df.to_csv(\"C:\\\\Users\\\\G20187729\\\\Desktop\\\\Python_codes_3\\\\Data\\\\p_values.csv\", index=False)\n",
    "significant_features_df.to_csv(\"C:\\\\Users\\\\G20187729\\\\Desktop\\\\Python_codes_3\\\\Data\\\\significant_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf29f9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of dictionaries where each dictionary represents a row in the DataFrame\n",
    "rows = []\n",
    "for significant_tuple in significant_features:\n",
    "    row_dict = {}\n",
    "    for i, feature_index in enumerate(significant_tuple):\n",
    "        row_dict[f\"Feature{i+1}\"] = feature_index\n",
    "    rows.append(row_dict)\n",
    "\n",
    "# Creating the DataFrame\n",
    "significant_features_df = pd.DataFrame(rows)\n",
    "significant_features_df.to_csv(\"C:\\\\Users\\\\G20187729\\\\Desktop\\\\Python_codes_3\\\\Data\\\\significant_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0e488a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a72d45-3d2b-4450-9e00-8a76c3ff187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation based on percentile and z-score method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b59037-2d93-4a61-8eb0-f9b53063e0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def correlation_analysis(array1, array2, threshold_percentile=99):\n",
    "    correlations = np.zeros((array1.shape[1], array2.shape[1]))\n",
    "    p_values = np.zeros((array1.shape[1], array2.shape[1]))\n",
    "\n",
    "    for i in range(array1.shape[1]):\n",
    "        for j in range(array2.shape[1]):\n",
    "            correlation, p_value = pearson_correlation(array1[:, i], array2[:, j])\n",
    "            correlations[i, j] = correlation\n",
    "            p_values[i, j] = p_value\n",
    "\n",
    "    # Check if data is normally distributed\n",
    "    is_normally_distributed = check_normal_distribution(np.concatenate((array1, array2), axis=1))\n",
    "\n",
    "    # Set threshold based on normality\n",
    "    if is_normally_distributed:\n",
    "        threshold = np.percentile(correlations, threshold_percentile)\n",
    "    else:\n",
    "        threshold = np.percentile(abs(correlations), 100 - stats.norm.cdf(1.96) * 100)\n",
    "\n",
    "    # Rank features based on correlation coefficient\n",
    "    significant_features = np.where(abs(correlations) > threshold)\n",
    "\n",
    "    return correlations, p_values, significant_features\n",
    "\n",
    "def pearson_correlation(x, y):\n",
    "    correlation, p_value = stats.pearsonr(x, y)\n",
    "    return correlation, p_value\n",
    "\n",
    "def check_normal_distribution(data):\n",
    "    _, p_value = stats.normaltest(data)\n",
    "    return (p_value > 0.01).all()  # Check if all elements satisfy the condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1a2d1f-5e7d-4488-8903-598605407b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations, p_values, significant_features = correlation_analysis(Z, X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877d6ffd-b0e4-41f0-a726-5724151b7953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to CSV file\n",
    "correlations_df = pd.DataFrame(correlations)\n",
    "p_values_df = pd.DataFrame(p_values)\n",
    "significant_features_df = pd.DataFrame(significant_features, columns=[f\"Feature{i+1}\" for i in range(len(significant_features[0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea681d9-f66b-4f17-b74d-db404fab04d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(significant_features_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f6e406-0f5c-46e6-9eb9-49728e022358",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_df.to_csv(\"C:\\\\Users\\\\G20187729\\\\Desktop\\\\Python_codes_3\\\\Data\\\\correlations_normality_checked.csv\", index=False)\n",
    "p_values_df.to_csv(\"C:\\\\Users\\\\G20187729\\\\Desktop\\\\Python_codes_3\\\\Data\\\\p_values_normality_checked.csv\", index=False)\n",
    "significant_features_df.to_csv(\"C:\\\\Users\\\\G20187729\\\\Desktop\\\\Python_codes_3\\\\Data\\\\significant_features_normality_checked.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50678c9-3a1f-4c95-b9a9-f5d38821dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_normally_distributed = check_normal_distribution(Z)\n",
    "print(\"Is the data normally distributed?\", is_normally_distributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08a6517-7289-486d-bc0c-fe98cd57bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate -log10(p-values)\n",
    "neg_log_p_values = -np.log10(p_values)\n",
    "\n",
    "# Extract correlation coefficients\n",
    "correlation_coefficients = correlations.flatten()\n",
    "\n",
    "# Plot volcano plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(correlation_coefficients, neg_log_p_values, color='blue', alpha=0.5)\n",
    "plt.axhline(-np.log10(threshold), color='red', linestyle='--', label=f'Threshold = {threshold:.2f}')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.ylabel('-log10(p-value)')\n",
    "plt.title('Volcano Plot')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3651190-d60d-418f-b589-f6f7d6d757ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18be753-30c7-471a-b5e6-da320a4f1804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6386506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Koopman matrix\n",
    "koopman_matrix = model.koopman.koopman_operator.get_koopman_matrix()\n",
    "\n",
    "# Compute the eigenvectors and eigenvalues\n",
    "eigenvalues, eigenvectors = np.linalg.eig(koopman_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e424523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort eigenvectors based on eigenvalues\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]  # Sort in descending order\n",
    "eigenvalues = eigenvalues[sorted_indices]\n",
    "eigenvectors = eigenvectors[:, sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f216fb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the eigenvectors\n",
    "num_eigenvectors_to_plot = 50  # Change this according to the number of eigenvectors you want to plot\n",
    "for i in range(num_eigenvectors_to_plot):\n",
    "    plt.plot(eigenvectors[:, i], label=f'Eigenvector {i+1}')\n",
    "\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Eigenvectors of Koopman Operator')\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5ecfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(len(eigenvalues)), eigenvalues)\n",
    "plt.xlabel('Eigenvalue Index')\n",
    "plt.ylabel('Eigenvalue Magnitude')\n",
    "plt.title('Eigenvalues of Koopman Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ea264-a999-4f88-9ae0-810aa1adac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot eigenvalues on the complex plane\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(np.real(eigenvalues), np.imag(eigenvalues), color='blue', label='Eigenvalues')\n",
    "plt.axhline(y=0, color='black', linestyle='--', linewidth=0.5)  # Horizontal line at y=0 (real axis)\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.5)  # Vertical line at x=0 (imaginary axis)\n",
    "plt.xlabel('Real Part')\n",
    "plt.ylabel('Imaginary Part')\n",
    "plt.title('Eigenvalues on Complex Plane')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbd327f-a9a6-49ef-8970-fb62c4398df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretation\n",
    "print(\"Eigenvalues:\")\n",
    "for eigenvalue in eigenvalues:\n",
    "    print(f\"Real Part: {np.real(eigenvalue):.2f}, Imaginary Part: {np.imag(eigenvalue):.2f}\")\n",
    "\n",
    "# Interpretation guidelines (replace with your analysis)\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Eigenvalues with negative real parts represent stable modes.\")\n",
    "print(\"- Eigenvalues with positive real parts represent unstable modes.\")\n",
    "print(\"- Eigenvalues with zero real parts represent neutral modes.\")\n",
    "print(\"- Nonzero imaginary parts indicate oscillatory behavior.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66a8534-3be8-4ad3-8e5c-1f0d3aa77b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af37e49-df43-47c0-84ea-b8c10368c90a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c37c0f-5feb-4e4c-8e6f-58cdc8298c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44f713-fc63-47da-967a-693b5e3346c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_eigenvectors = eigenvectors / np.linalg.norm(eigenvectors, axis=0)\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(normalized_eigenvectors[:, :num_eigenvectors_to_plot], cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Normalized Value')\n",
    "plt.xlabel('Eigenvector Index')\n",
    "plt.ylabel('State Index')\n",
    "plt.title('Heatmap of Eigenvectors of Koopman Operator')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d993bb99-39cb-4843-8f7d-72927e7d1860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotEignValues(model):\n",
    "    '''\n",
    "    Plots the eigen values of the learned Koopman operator\n",
    "    Args:\n",
    "        args (argparse): object with programs arguements\n",
    "        model (tensorflow.keras.Model): tensorflow model with koopman operator kMatrix\n",
    "    '''\n",
    "    # Get koopman operator from model\n",
    "    kMatrix = model.koopman.koopman_operator.get_koopman_matrix().numpy()\n",
    "\n",
    "    try:\n",
    "        w, v = np.linalg.eig(kMatrix)\n",
    "    except:\n",
    "        print('issue computing eigs')\n",
    "        return\n",
    "\n",
    "    plt.close('all')\n",
    "    plt.scatter(np.real(np.log(np.abs(w))), np.imag(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac740b89-d7d4-4b9f-abf5-837a4de454e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotEignValues(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d081eb4-f4dc-49e4-9b90-fdd339281a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotEignVectors(model, n=3):\n",
    "    '''\n",
    "    Plots the eigen vectors of the learned Koopman operator\n",
    "    Args:\n",
    "        args (argparse): object with programs arguements\n",
    "        model (tensorflow.keras.Model): tensorflow model with koopman operator kMatrix\n",
    "    '''\n",
    "    # Get koopman operator from model\n",
    "    kMatrix = model.koopman.koopman_operator.get_koopman_matrix().numpy()\n",
    "    w, v = np.linalg.eig(kMatrix)\n",
    "    idx = np.argsort(w)[::-1]\n",
    "    w = w[idx]\n",
    "    v = v[:,idx]\n",
    "\n",
    "    data = test_loader\n",
    "    yPred = tf.constant(data[:,:,:-1], dtype=tf.float32)\n",
    "\n",
    "    plt.close(\"all\")\n",
    "    fig, ax = plt.subplots(1, n, figsize=(4*n, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ee312-bbf3-46b9-a826-ffe1bcebd75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotEignVectors(model, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83858e37-c52b-4654-973b-1d6aa04c9ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0731e94-6c15-4b7a-a606-9442c72478d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b760756-d280-4086-8aed-3cee2c072344",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = np.concatenate([X0_train_scaled, X1_train_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7a9f8f-8a25-416b-9845-769e3d615aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_latent = np.concatenate([Z_t, Z_t1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d03cc29-55f6-45f3-b3bb-215f7989f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_latent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95e2b71-55f7-40a6-878e-b5c2e5d670d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def correlation_analysis(array1, array2, threshold_percentile=99):\n",
    "    correlations = np.zeros((array1.shape[1], array2.shape[1]))\n",
    "    p_values = np.zeros((array1.shape[1], array2.shape[1]))\n",
    "\n",
    "    for i in range(array1.shape[1]):\n",
    "        for j in range(array2.shape[1]):\n",
    "            correlation, p_value = pearson_correlation(array1[:, i], array2[:, j])\n",
    "            correlations[i, j] = correlation\n",
    "            p_values[i, j] = p_value\n",
    "\n",
    "    # Check if data is normally distributed\n",
    "    is_normally_distributed = check_normal_distribution(np.concatenate((array1, array2), axis=1))\n",
    "\n",
    "    # Set threshold based on normality\n",
    "    if is_normally_distributed:\n",
    "        threshold = np.percentile(correlations, threshold_percentile)\n",
    "    else:\n",
    "        threshold = np.percentile(abs(correlations), 100 - stats.norm.cdf(1.96) * 100)\n",
    "\n",
    "    # Rank features based on correlation coefficient\n",
    "    significant_features = np.where(abs(correlations) > threshold)\n",
    "\n",
    "    return correlations, p_values, significant_features\n",
    "\n",
    "def pearson_correlation(x, y):\n",
    "    correlation, p_value = stats.pearsonr(x, y)\n",
    "    return correlation, p_value\n",
    "\n",
    "def check_normal_distribution(data):\n",
    "    _, p_value = stats.normaltest(data)\n",
    "    return (p_value > 0.01).all()  # Check if all elements satisfy the condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2478a624-7078-495b-b490-e82c77e26035",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations, p_values, significant_features = correlation_analysis(combined_data, combined_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a622805-8965-474c-aae5-ba4cd7eb6c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correlations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067c1db-0502-4fde-b54a-e0d19cd4cad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators = 100)\n",
    "rf.fit(combined_data, combined_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d168c99-831c-4016-a304-c597ecc1fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246ac62d-25ce-4e20-8534-d03766d4fdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84f29bc-d8db-40fe-b637-0dde33c77bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(feature_importances)), feature_importances, color='skyblue')\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Feature Importance\")\n",
    "plt.title('Feature Importance for Predicting Latent Space')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69479cda-1e3e-4f2c-9b1b-b99d80ffb313",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlations, cmap='coolwarm', xticklabels=False)\n",
    "plt.title('Correlation between Features and Latent Space')\n",
    "plt.xlabel('Latent Space')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e65cd-9559-490c-aaa4-320ca585f11c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key, indices in grouped_indices.items():\n",
    "    print(\"Group\", key, \":\")\n",
    "    for index in indices:\n",
    "        print(\"Feature index:\", index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff2f9bd-7e34-46c0-a49a-8d39859b77ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1_array = np.array(grouped_indices[1])\n",
    "print(\"Shape of Group 1 array:\", group_1_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7484ac8-e0f8-486e-ac65-d2653fe3d0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Define the directory where CSV files will be saved\n",
    "output_directory = \"C:\\\\Users\\\\G20187729\\\\Desktop\\\\Python_codes_3\\\\Data\\\\correlations\"\n",
    "\n",
    "# Assuming grouped_indices is a dictionary containing groups of feature indices\n",
    "\n",
    "# Define a function to write a group to a CSV file\n",
    "def write_group_to_csv(group_indices, group_number):\n",
    "    filename = os.path.join(output_directory, f\"Group_{group_number}_indices.csv\")\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"Feature Index\"])\n",
    "        for index in group_indices:\n",
    "            writer.writerow([index])\n",
    "\n",
    "# Write each group to a separate CSV file\n",
    "for group_number, indices in grouped_indices.items():\n",
    "    write_group_to_csv(indices, group_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ab86df-d3b9-4d34-abd2-90fcee9a6b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correlations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3f8027-7dbd-4565-be92-4650d447cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_features_df = pd.DataFrame(significant_features, columns=[f\"Feature{i+1}\" for i in range(len(significant_features[0]))])\n",
    "print(significant_features_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db7d6a-bb39-41cc-a822-589be3f66aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b2893a-8d24-457d-a6c5-e06733dcf7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to store CSV files\n",
    "directory = \"significant_feature_groups\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Create a DataFrame with significant_features\n",
    "df = pd.DataFrame({\n",
    "    'Feature1': significant_features[1],  # Assuming Feature1 is the first array in significant_features\n",
    "    'Feature2': significant_features[0]   # Assuming Feature2 is the second array in significant_features\n",
    "})\n",
    "\n",
    "# Group by unique values of Feature1\n",
    "groups = df.groupby('Feature1')\n",
    "\n",
    "# Iterate over groups and create individual CSV files\n",
    "for group_name, group_data in groups:\n",
    "    filename = f\"group_{group_name}.csv\"\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    group_data[['Feature2']].to_csv(filepath, index=False)  # Save only Feature2 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610bdadf-bd26-4559-8813-c600b83f5471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def correlation_worker(i, j, array1, array2):\n",
    "    correlation, p_value = spearmanr(array1[:, i], array2[:, j])\n",
    "    return correlation, p_value\n",
    "\n",
    "def correlation_analysis_parallel(array1, array2, threshold_percentile=99):\n",
    "    num_cores = -1  # Use all available CPU cores\n",
    "\n",
    "    correlations = np.zeros((array1.shape[1], array2.shape[1]))\n",
    "    p_values = np.zeros((array1.shape[1], array2.shape[1]))\n",
    "\n",
    "    results = Parallel(n_jobs=num_cores)(\n",
    "        delayed(correlation_worker)(i, j, array1, array2)\n",
    "        for i in range(array1.shape[1]) for j in range(array2.shape[1])\n",
    "    )\n",
    "\n",
    "    for (i, j), (correlation, p_value) in zip(np.ndindex(correlations.shape), results):\n",
    "        correlations[i, j] = correlation\n",
    "        p_values[i, j] = p_value\n",
    "\n",
    "    # Set threshold based on normality\n",
    "    threshold = np.percentile(abs(correlations), threshold_percentile)\n",
    "\n",
    "    # Rank features based on correlation coefficient\n",
    "    significant_features = np.where(abs(correlations) > threshold)\n",
    "\n",
    "    return correlations, p_values, significant_features\n",
    "\n",
    "# Assuming you have prepared your data arrays: array1 and array2\n",
    "correlations, p_values, significant_features = correlation_analysis_parallel(combined_data, combined_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07491915-573a-4b2c-8135-769286ade55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correlations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3041fe-5e50-4059-9523-9675510b5b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_features_df = pd.DataFrame(significant_features, columns=[f\"Feature{i+1}\" for i in range(len(significant_features[0]))])\n",
    "print(significant_features_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e8f3a2-f339-41ff-94f2-fcaf20672013",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1cb434-7f9f-4326-970b-38841734e42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Directory to store CSV files\n",
    "directory = \"significant_feature_groups\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Create a DataFrame with significant_features\n",
    "df = pd.DataFrame({\n",
    "    'Feature1': significant_features[1],  # Assuming Feature1 is the first array in significant_features\n",
    "    'Feature2': significant_features[0]   # Assuming Feature2 is the second array in significant_features\n",
    "})\n",
    "\n",
    "# Group by unique values of Feature1\n",
    "groups = df.groupby('Feature1')\n",
    "\n",
    "# Iterate over groups and create individual CSV files\n",
    "for group_name, group_data in groups:\n",
    "    filename = f\"group_{group_name}.csv\"\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    group_data[['Feature2']].to_csv(filepath, index=False)  # Save only Feature2 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859734c0-6fcf-4afc-be3e-858bacf40e93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
