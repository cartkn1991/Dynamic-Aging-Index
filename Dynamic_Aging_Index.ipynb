{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5baa51-29fd-49d4-9d17-ddbaa0616086",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the necessary libraries\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda, BatchNormalization, Add, Dropout\n",
    "from tensorflow.keras import layers, metrics, models, regularizers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow import keras\n",
    "import joblib\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d28e83a-8113-4095-b942-a80880c07671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the datasets\n",
    "\n",
    "def print_title(title):\n",
    "    print(f'{50 * \"=\"}')\n",
    "    print(title)\n",
    "    print(f'{50 * \"=\"}')\n",
    "\n",
    "\n",
    "print_title('Loading Data')\n",
    "df = pq.read_table(\"C:\\\\Users\\\\Data\\\\crosssectionaldata\\\\X.parquet\", use_threads=True).to_pandas()\n",
    "X0_train = pq.read_table(\"C:\\\\Users\\\\Data\\\\X0_train.parquet\", use_threads=True).to_pandas()\n",
    "X0_test = pq.read_table(\"C:\\\\Users\\\\Data\\\\X0_test.parquet\", use_threads=True).to_pandas()\n",
    "X1_train = pq.read_table(\"C:\\\\Users\\\\Data\\\\X1_train.parquet\", use_threads=True).to_pandas()\n",
    "X1_test = pq.read_table(\"C:\\\\Users\\\\Data\\\\X1_test.parquet\", use_threads=True).to_pandas()\n",
    "print_title('Data loading complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f710c-4588-4367-a422-3da81320d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process and split the datasets for validation\n",
    "\n",
    "X = df.iloc[:, 1:, ]\n",
    "y1 = df.iloc[:, 0]\n",
    "\n",
    "y = X.iloc[:, 0]\n",
    "X = X.iloc[:, 1:, ]\n",
    "\n",
    "X = X.values.astype(np.float32)\n",
    "X0_train = X0_train.values.astype(np.float32)\n",
    "X1_train = X1_train.values.astype(np.float32)\n",
    "X0_test = X0_test.values.astype(np.float32)\n",
    "X1_test = X1_test.values.astype(np.float32)\n",
    "\n",
    "X_train, X_test, y_train, y_test, y1_train, y1_test = train_test_split(X, y, y1, test_size=0.2, random_state=0)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
    "X0_train, X0_val = train_test_split(X0_train, test_size=0.2, random_state=0, shuffle=False)\n",
    "X1_train, X1_val = train_test_split(X1_train, test_size=0.2, random_state=0, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28846140-97eb-45e2-b778-b06a0e536fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the datasets\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform the training set\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the validation and test sets using the same scaler\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Similarly, fit and transform X0_train, X0_val, X0_test for the second dataset\n",
    "X0_train_scaled = scaler.transform(X0_train)\n",
    "X0_val_scaled = scaler.transform(X0_val)\n",
    "X0_test_scaled = scaler.transform(X0_test)\n",
    "\n",
    "# And fit and transform X1_train, X1_val, X1_test for the third dataset\n",
    "X1_train_scaled = scaler.transform(X1_train)\n",
    "X1_val_scaled = scaler.transform(X1_val)\n",
    "X1_test_scaled = scaler.transform(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361f653d-d77d-457d-b476-707c0f44ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c68fe9-ae6f-4b47-8cb4-396d001145cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (332909,)\n",
    "latent_dim = 25\n",
    "batch_size = 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b2feec-6efb-435b-92bd-dc2c9baa2eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Autoencoder Architecture\n",
    "\n",
    "def residual_block(x, filters, kernel_size=3, stride=1, l1_reg=0.01, l2_reg=0.01, dropout_rate=0.4, initializer='he_normal'):\n",
    "    shortcut = x\n",
    "    x = layers.Dense(filters, activation=\"relu\", \n",
    "                     kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg),\n",
    "                     kernel_initializer=initializer)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(filters, activation=None, \n",
    "                     kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg),\n",
    "                     kernel_initializer=initializer)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Adjust the shortcut if necessary\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Dense(filters, activation=None, \n",
    "                                kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg),\n",
    "                                kernel_initializer=initializer)(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "    \n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_encoder(input_shape, latent_dim, l1_reg=0.01, l2_reg=0.01, dropout_rate=0.4, initializer='he_normal'):\n",
    "    encoder_inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Dense(600, activation=\"relu\", \n",
    "                     kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg),\n",
    "                     kernel_initializer=initializer)(encoder_inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Add residual blocks\n",
    "    x = residual_block(x, 600, l1_reg=l1_reg, l2_reg=l2_reg, initializer=initializer)\n",
    "    x = residual_block(x, 400, l1_reg=l1_reg, l2_reg=l2_reg, dropout_rate=dropout_rate, initializer=initializer)\n",
    "    x = residual_block(x, 200, l1_reg=l1_reg, l2_reg=l2_reg, initializer=initializer)\n",
    "    x = residual_block(x, 100, l1_reg=l1_reg, l2_reg=l2_reg, dropout_rate=dropout_rate, initializer=initializer)\n",
    "    x = residual_block(x, 50, l1_reg=l1_reg, l2_reg=l2_reg, dropout_rate=dropout_rate)\n",
    "        \n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\", kernel_initializer=initializer)(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\", kernel_initializer=initializer)(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    encoder = models.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "    return encoder\n",
    "\n",
    "\n",
    "encoder = build_encoder(input_shape, latent_dim, l1_reg=0.01, l2_reg=0.01, dropout_rate=0.4, initializer='he_normal')\n",
    "\n",
    "def build_decoder(latent_dim, output_shape, l1_reg=0.01, l2_reg=0.01, dropout_rate=0.4, initializer='he_normal'):\n",
    "    latent_inputs = layers.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(50, activation=\"relu\", \n",
    "                     kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg),\n",
    "                     kernel_initializer=initializer)(latent_inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Add residual blocks\n",
    "    \n",
    "    x = residual_block(x, 50, l1_reg=l1_reg, l2_reg=l2_reg, initializer=initializer)\n",
    "    x = residual_block(x, 100, l1_reg=l1_reg, l2_reg=l2_reg, dropout_rate=dropout_rate, initializer=initializer)\n",
    "    x = residual_block(x, 200, l1_reg=l1_reg, l2_reg=l2_reg, initializer=initializer)\n",
    "    x = residual_block(x, 400, l1_reg=l1_reg, l2_reg=l2_reg, dropout_rate=dropout_rate, initializer=initializer)\n",
    "    x = residual_block(x, 600, l1_reg=l1_reg, l2_reg=l2_reg, initializer=initializer)\n",
    "    \n",
    "    decoder_outputs = layers.Dense(output_shape, activation=\"sigmoid\", kernel_initializer=initializer)(x)  # Use linear activation for real-valued outputs\n",
    "    decoder = models.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "    return decoder\n",
    "\n",
    "\n",
    "decoder = build_decoder(latent_dim, 332909, l1_reg=0.01, l2_reg=0.01, dropout_rate=0.4, initializer='he_normal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ef5c2d-7d82-451d-b56e-4bfc1b0806f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Projector block where Latent space is reduced to 1D scalar value - DAI\n",
    "\n",
    "class ScalarTransformation(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ScalarTransformation, self).__init__(**kwargs)\n",
    "        self.dense = layers.Dense(1, activation=None)  # Single scalar output\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f394bc8-f87c-473b-af77-c97659e984cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variational autoencoder class\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, scalar_transformation, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.scalar_transformation = scalar_transformation\n",
    "      \n",
    "\n",
    "    def call(self, inputs):\n",
    "        print(len(inputs))\n",
    "        z_mean_cross, z_log_var_cross, z_cross = self.encoder(inputs[0])\n",
    "        z_mean_present, z_log_var_present, z_present = self.encoder(inputs[1])\n",
    "        z_mean_future, z_log_var_future, z_future = self.encoder(inputs[2])\n",
    "\n",
    "        scalar_present = self.scalar_transformation(z_present)\n",
    "        scalar_future = self.scalar_transformation(z_future)\n",
    "        \n",
    "        reconstruction = self.decoder(z_cross)\n",
    "\n",
    "        return reconstruction, z_present, z_future, z_mean_cross, z_log_var_cross, scalar_present, scalar_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb0dcd5-1f9f-43f4-b84d-510508e2d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Koopman operator class\n",
    "\n",
    "class KoopmanOperator(tf.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.latent_dim = params['latent_dim']  # Use latent_dim directly from params, no default\n",
    "        \n",
    "        # Create all omega networks once during initialization\n",
    "        self.omega_nets = self.create_all_omega_nets()\n",
    "        \n",
    "        # Create transformation layer once during initialization\n",
    "        self.transformation_layer = layers.Dense(1, activation=None)\n",
    "    \n",
    "    def form_complex_conjugate_block(self, omegas, delta_t):\n",
    "        scale = tf.exp(omegas[:, 1] * delta_t)\n",
    "        entry11 = tf.multiply(scale, tf.cos(omegas[:, 0] * delta_t))\n",
    "        entry12 = tf.multiply(scale, tf.sin(omegas[:, 0] * delta_t))\n",
    "        row1 = tf.stack([entry11, -entry12], axis=1)  # [None, 2]\n",
    "        row2 = tf.stack([entry12, entry11], axis=1)  # [None, 2]\n",
    "        result = tf.stack([row1, row2], axis=2)\n",
    "        print(\"form_complex_conjugate_block - result shape:\", result.shape)\n",
    "        return result\n",
    "\n",
    "    def varying_multiply(self, y, omegas, delta_t):\n",
    "        num_real = self.params.get('num_real', 0)\n",
    "        num_complex_pairs = self.params.get('num_complex_pairs', 0)\n",
    "        complex_list = []\n",
    "        real_list = []\n",
    "\n",
    "        for j in range(num_complex_pairs):\n",
    "            ind = 2 * j\n",
    "            ystack = tf.stack([y[:, ind:ind + 2], y[:, ind:ind + 2]], axis=2)  # [None, 2, 2]\n",
    "            L_stack = self.form_complex_conjugate_block(omegas[j], delta_t)\n",
    "            elmtwise_prod = tf.multiply(ystack, L_stack)\n",
    "            complex_list.append(tf.reduce_sum(elmtwise_prod, 1))\n",
    "\n",
    "        if len(complex_list) > 0:\n",
    "            complex_part = tf.concat(complex_list, axis=1)\n",
    "            print(\"varying_multiply - complex_part shape:\", complex_part.shape)\n",
    "\n",
    "        for j in range(num_real):\n",
    "            ind = 2 * num_complex_pairs + j\n",
    "            temp = y[:, ind]\n",
    "            real_list.append(tf.multiply(temp[:, tf.newaxis], tf.exp(omegas[num_complex_pairs + j] * delta_t)))\n",
    "\n",
    "        if len(real_list) > 0:\n",
    "            real_part = tf.concat(real_list, axis=1)\n",
    "            print(\"varying_multiply - real_part shape:\", real_part.shape)\n",
    "\n",
    "        # Ensure the final result has the correct shape\n",
    "        if len(complex_list) > 0 and len(real_list) > 0:\n",
    "            result = tf.concat([complex_part, real_part], axis=1)\n",
    "            result = result[:, :self.latent_dim]  # Trim to latent_dim\n",
    "            print(\"varying_multiply - result shape (complex + real):\", result.shape)\n",
    "            return result\n",
    "        elif len(complex_list) > 0:\n",
    "            return complex_part\n",
    "        else:\n",
    "            return real_part\n",
    "        \n",
    "    def create_all_omega_nets(self):\n",
    "        omega_nets = []\n",
    "        for j in range(self.params['num_complex_pairs']):\n",
    "            temp_name = f'OC{j + 1}'\n",
    "            omega_net = self.create_one_omega_net(temp_name)  # Create model\n",
    "            omega_nets.append(omega_net)\n",
    "    \n",
    "        for j in range(self.params['num_real']):\n",
    "            temp_name = f'OR{j + 1}'\n",
    "            omega_net = self.create_one_omega_net(temp_name)  # Create model\n",
    "            omega_nets.append(omega_net)\n",
    "    \n",
    "        print(\"create_all_omega_nets - number of omega_nets:\", len(omega_nets))\n",
    "        return omega_nets\n",
    "\n",
    "    def create_one_omega_net(self, name_prefix):\n",
    "        latent_inputs = layers.Input(shape=(self.latent_dim,))\n",
    "        \n",
    "        x = layers.Dense(8, activation=\"relu\", name=f'{name_prefix}_dense1')(latent_inputs)\n",
    "        x = layers.BatchNormalization(name=f'{name_prefix}_batchnorm1')(x)\n",
    "        x = layers.Dropout(0.4, name=f'{name_prefix}_dropout1')(x)       \n",
    "        \n",
    "        x = residual_block(x, 8, l1_reg=0.01, l2_reg=0.01)\n",
    "        x = residual_block(x, 4, l1_reg=0.01, l2_reg=0.01, dropout_rate=0.4)\n",
    "        x = residual_block(x, 2, l1_reg=0.01, l2_reg=0.01)\n",
    "        \n",
    "        omega_params = layers.Dense(self.latent_dim, name=f'{name_prefix}_output')(x)\n",
    "        omegas = tf.keras.Model(latent_inputs, omega_params, name=name_prefix)\n",
    "        \n",
    "        return omegas\n",
    "\n",
    "    def apply_omega_nets(self, ycoords):\n",
    "        omegas = []\n",
    "        for j in range(self.params['num_complex_pairs']):\n",
    "            ind = 2 * j\n",
    "            pair_of_columns = ycoords[:, ind:ind + 2]\n",
    "            radius_of_pair = tf.reduce_sum(tf.square(pair_of_columns), axis=1, keepdims=True)\n",
    "            radius_of_pair = tf.tile(radius_of_pair, [1, self.latent_dim])\n",
    "            omega_output = self.omega_nets[j](radius_of_pair)\n",
    "            print(f\"apply_omega_nets - omega_net {j} output shape:\", omega_output.shape)\n",
    "            omegas.append(omega_output)\n",
    "    \n",
    "        for j in range(self.params['num_real']):\n",
    "            ind = 2 * self.params['num_complex_pairs'] + j\n",
    "            one_column = ycoords[:, ind]\n",
    "            one_column = tf.tile(one_column[:, tf.newaxis], [1, self.latent_dim])\n",
    "            omega_output = self.omega_nets[self.params['num_complex_pairs'] + j](one_column)\n",
    "            print(f\"apply_omega_nets - omega_net {self.params['num_complex_pairs'] + j} output shape:\", omega_output.shape)\n",
    "            omegas.append(omega_output)\n",
    "    \n",
    "        return omegas\n",
    "\n",
    "    def compute_future_state(self, current_state, delta_t):\n",
    "        \"\"\"\n",
    "        Compute future state based on current state and varying delta_t.\n",
    "        \"\"\"\n",
    "        ycoords = current_state\n",
    "        omegas = self.apply_omega_nets(ycoords)\n",
    "        \n",
    "        # Adjust varying time steps (delta_t)\n",
    "        future_state = self.varying_multiply(current_state, omegas, delta_t)\n",
    "        \n",
    "        # Apply transformation to future state\n",
    "        trans_future_state = self.transformation_layer(future_state)\n",
    "        \n",
    "        return future_state, trans_future_state\n",
    "\n",
    "\n",
    "\n",
    "# Example parameters for model creation\n",
    "params = {\n",
    "    'input_shape': (332909,),  # Example input shape\n",
    "    'latent_dim': 25,          # Latent space dimension\n",
    "    'l1_reg': 0.01,            # L1 regularization strength\n",
    "    'l2_reg': 0.01,            # L2 regularization strength\n",
    "    'dropout_rate': 0.4,       # Dropout rate\n",
    "    'delta_t': 3,              # Time step size\n",
    "    'num_real': 15,            # Number of real eigenvalues\n",
    "    'num_complex_pairs': 5,   # Number of complex conjugate eigenvalue pairs\n",
    "    'output_shape': 332909,    # Output shape\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac5c356-c07f-4ed4-9447-091ec987455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the KoopmanModel class\n",
    "\n",
    "class KoopmanModel(tf.keras.Model):\n",
    "    def __init__(self, koopman_operator):\n",
    "        super(KoopmanModel, self).__init__()\n",
    "        self.koopman_operator = koopman_operator\n",
    "\n",
    "    def call(self, input_present, num_future=1, time_intervals=None):\n",
    "        future_states = []\n",
    "        trans_future_states = []\n",
    "        current_state = input_present\n",
    "        \n",
    "        # If no custom time intervals are provided, use a fixed delta_t\n",
    "        if time_intervals is None:\n",
    "            time_intervals = [self.koopman_operator.params['delta_t']] * num_future\n",
    "        \n",
    "        for i in range(num_future):\n",
    "            delta_t = time_intervals[i]  # Use the appropriate delta_t for each step\n",
    "            g_next_state, trans_future_state = self.koopman_operator.compute_future_state(current_state, delta_t)\n",
    "            \n",
    "            future_states.append(g_next_state)\n",
    "            trans_future_states.append(trans_future_state)\n",
    "            \n",
    "            current_state = g_next_state  # Update current state to the newly predicted state\n",
    "        \n",
    "        return future_states, trans_future_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a0dc33-6279-4576-b7f1-fc0ea4732968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The entire architecture with custom loss functions and gradient initiation\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vae, koopman, loss_weights=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vae = vae\n",
    "        self.koopman = koopman        \n",
    "        self.total_loss_tracker = metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = metrics.Mean(name=\"kl_loss\")\n",
    "        self.linear_dynamics_loss_tracker = metrics.Mean(name=\"linear_dynamics_loss\")\n",
    "        self.future_state_loss_tracker = metrics.Mean(name=\"future_state_loss\")\n",
    "        self.aux_loss_tracker = metrics.Mean(name=\"aux_loss\")\n",
    "        self.l_inf_loss_tracker = metrics.Mean(name=\"l_inf_loss\")\n",
    "\n",
    "        if loss_weights is None:\n",
    "            loss_weights = {\n",
    "                \"reconstruction_loss\": 10.0,\n",
    "                \"kl_loss\": 1.0,\n",
    "                \"linear_dynamics_loss\": 100.0,\n",
    "                \"future_state_loss\": 100.0,\n",
    "                \"l_inf_loss\": 1.0\n",
    "            }\n",
    "        self.loss_weights = loss_weights\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "            self.linear_dynamics_loss_tracker,\n",
    "            self.future_state_loss_tracker,\n",
    "            self.aux_loss_tracker,\n",
    "            self.l_inf_loss_tracker\n",
    "        ]\n",
    "    \n",
    "    def call(self, inputs, num_future=1, time_intervals=None):\n",
    "        \"\"\"\n",
    "        Call method adjusted to handle multiple future time intervals (3 years and 10 years).\n",
    "        \"\"\"\n",
    "        reconstruction, z_present, z_future, z_mean_cross, z_log_var_cross, scalar_present, scalar_future = self.vae(inputs)\n",
    "        \n",
    "        future_states = []\n",
    "        trans_future_states = []\n",
    "        \n",
    "        current_state = z_present\n",
    "        \n",
    "        # Adjust for varying time intervals\n",
    "        if time_intervals is None:\n",
    "            time_intervals = [self.koopman.koopman_operator.params['delta_t']] * num_future\n",
    "        \n",
    "        for i in range(num_future):\n",
    "            # Use the respective time interval for each future step\n",
    "            delta_t = time_intervals[i]\n",
    "            g_next_state, trans_future_state = self.koopman.koopman_operator.compute_future_state(current_state, delta_t)\n",
    "            future_states.append(g_next_state)\n",
    "            trans_future_states.append(trans_future_state)\n",
    "            \n",
    "            # Update current state to the newly predicted state\n",
    "            current_state = g_next_state\n",
    "\n",
    "        aux_reconstructed = [self.vae.decoder(g_next) for g_next in future_states]\n",
    "        \n",
    "        # Return the states\n",
    "        return reconstruction, z_present, z_future, z_mean_cross, z_log_var_cross, scalar_present, scalar_future, future_states, trans_future_states, aux_reconstructed\n",
    "\n",
    "\n",
    "    def compute_losses(self, inputs, reconstruction, z_present, z_future, z_mean_cross, z_log_var_cross, scalar_present, scalar_future, k_trans, k_untrans, aux_reconstructed, num_future=1):\n",
    "        input_data_cross, input_data_present, input_data_future = inputs\n",
    "    \n",
    "        # Reconstruction loss (L_recon)\n",
    "        reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.square(input_data_cross - reconstruction), axis=-1))\n",
    "        \n",
    "        # KL divergence loss (remains as is)\n",
    "        kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var_cross - tf.square(z_mean_cross) - tf.exp(z_log_var_cross))\n",
    "    \n",
    "        # Future state prediction loss (L_pred)\n",
    "        linear_dynamics_loss = 0.0\n",
    "        for i in range(num_future):\n",
    "            linear_dynamics_loss += tf.reduce_mean(tf.reduce_sum(tf.square(k_untrans[i] - z_future[i]), axis=-1))\n",
    "\n",
    "\n",
    "        future_state_loss = 0.0\n",
    "        for i in range(num_future):\n",
    "            future_state_loss += tf.reduce_mean(tf.reduce_sum(tf.square(k_trans[i] - scalar_future[i]), axis=-1))\n",
    "\n",
    "\n",
    "        aux_loss = tf.reduce_mean(tf.reduce_sum(tf.square(aux_reconstructed[-1] - input_data_future), axis=-1))\n",
    "    \n",
    "        # Infinity Norm Loss (L_inf)\n",
    "        l_inf_loss = tf.reduce_max(tf.abs(input_data_cross - reconstruction)) + tf.reduce_max(tf.abs(input_data_future - aux_reconstructed[-1]))\n",
    "    \n",
    "        # Apply loss weights\n",
    "        total_loss = (\n",
    "            self.loss_weights[\"reconstruction_loss\"] * (reconstruction_loss + aux_loss) +\n",
    "            self.loss_weights[\"linear_dynamics_loss\"] * linear_dynamics_loss +  \n",
    "            self.loss_weights[\"kl_loss\"] * kl_loss +\n",
    "            self.loss_weights[\"future_state_loss\"] * future_state_loss +\n",
    "            self.loss_weights[\"l_inf_loss\"] * l_inf_loss\n",
    "        )\n",
    "        \n",
    "        return total_loss, reconstruction_loss, kl_loss, linear_dynamics_loss, future_state_loss, aux_loss, l_inf_loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        data_unpacked = data[0]\n",
    "        input_data_cross, input_data_present, input_data_future = data_unpacked\n",
    "        \n",
    "        # Define time intervals: 3-year and 10-year prediction\n",
    "        time_intervals = [3]  # Use [3] for single-step prediction or modify as needed\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstruction, z_present, z_future, z_mean_cross, z_log_var_cross, scalar_present, scalar_future, k_untrans, k_trans, aux_reconstructed = self(\n",
    "                data_unpacked, num_future=len(time_intervals), time_intervals=time_intervals, training=True\n",
    "            )\n",
    "        \n",
    "            # Compute losses\n",
    "            total_loss, reconstruction_loss, kl_loss, linear_dynamics_loss, future_state_loss, aux_loss, l_inf_loss = self.compute_losses(\n",
    "                (input_data_cross, input_data_present, input_data_future),\n",
    "                reconstruction, z_present, z_future, z_mean_cross, z_log_var_cross, scalar_present, scalar_future, k_trans, k_untrans, aux_reconstructed, num_future=len(time_intervals)\n",
    "            )\n",
    "        \n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(total_loss, self.trainable_variables)\n",
    "        \n",
    "        # Apply gradients\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # Update metrics\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        self.linear_dynamics_loss_tracker.update_state(linear_dynamics_loss)\n",
    "        self.future_state_loss_tracker.update_state(future_state_loss)\n",
    "        self.aux_loss_tracker.update_state(aux_loss)\n",
    "        self.l_inf_loss_tracker.update_state(l_inf_loss)\n",
    "        \n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        data_unpacked = data[0]\n",
    "        input_data_cross, input_data_present, input_data_future = data_unpacked\n",
    "        \n",
    "        # Define time intervals for testing: e.g., 3-year and 10-year prediction\n",
    "        time_intervals = [3]\n",
    "        \n",
    "        reconstruction, z_present, z_future, z_mean_cross, z_log_var_cross, scalar_present, scalar_future, k_trans, k_untrans, aux_reconstructed = self(\n",
    "            data_unpacked, num_future=len(time_intervals), time_intervals=time_intervals, training=False\n",
    "        )\n",
    "        \n",
    "        # Compute losses\n",
    "        total_loss, reconstruction_loss, kl_loss, linear_dynamics_loss, future_state_loss, aux_loss, l_inf_loss = self.compute_losses(\n",
    "            (input_data_cross, input_data_present, input_data_future),\n",
    "            reconstruction, z_present, z_future, z_mean_cross, z_log_var_cross, scalar_present, scalar_future, k_trans, k_untrans, aux_reconstructed, num_future=len(time_intervals)\n",
    "        )\n",
    "        \n",
    "        # Update metrics\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        self.linear_dynamics_loss_tracker.update_state(linear_dynamics_loss)\n",
    "        self.future_state_loss_tracker.update_state(future_state_loss)\n",
    "        self.aux_loss_tracker.update_state(aux_loss)\n",
    "        self.l_inf_loss_tracker.update_state(l_inf_loss)\n",
    "        \n",
    "        return {m.name: m.result() for m in self.metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61448fb-18d4-4fe0-961f-1968ea6a6738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90dcd59-addf-4438-bb90-c7eb56cf501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch % 250 == 0 and epoch != 0:\n",
    "        return lr * 0.1  # reduce learning rate by a factor of 10\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "# Create a learning rate scheduler callback\n",
    "lr_scheduler_callback = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "model.save_weights(filepath.format(epoch=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a06548-4674-464e-bc61-f9393094cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    import time\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    scalar_transformation = ScalarTransformation()    \n",
    "    vae = VAE(encoder, decoder, scalar_transformation)\n",
    "    koopman_operator = KoopmanOperator(params)\n",
    "    koopman = KoopmanModel(koopman_operator)\n",
    "    \n",
    "    model = MyModel(vae, koopman)\n",
    "    checkpoint_path = 'C:\\\\Users\\\\Best model\\\\saved-model-{epoch:02d}DAF.ckpt'\n",
    "    checkpoint = tf.train.Checkpoint(model=model)\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate = 0.0001, clipvalue=1.0, clipnorm=1.0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d24ae2-69c5-468a-b684-893680099f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Begins the training\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    hist = model.fit(\n",
    "        train_loader,\n",
    "        epochs=1000,\n",
    "        validation_data=val_loader,\n",
    "        validation_freq=1,\n",
    "        callbacks=[checkpoint]\n",
    "    )\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(f'Training time: {hms_string(elapsed)}')\n",
    "#     print(hist.history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
